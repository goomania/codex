{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc62a7c",
   "metadata": {},
   "source": [
    "# Computer Vision Mini Exercises\n",
    "\n",
    "Welcome to this mini set of computer vision exercises! The notebook is designed for quick demonstrations in a classroom setting and focuses on **image classification** using both **PyTorch** and **TensorFlow**.\n",
    "\n",
    "You will:\n",
    "\n",
    "1. Install the required libraries (PyTorch, TorchVision, and TensorFlow).\n",
    "2. Explore the Fashion-MNIST dataset.\n",
    "3. Train and evaluate a compact convolutional neural network (CNN) in PyTorch.\n",
    "4. Repeat a similar experiment with TensorFlow/Keras.\n",
    "5. Compare the predictions from both frameworks.\n",
    "\n",
    "> **Tip for Colab:** Runtime → Change runtime type → Hardware accelerator → GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b432b3",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "If you are running this notebook in Google Colab, execute the cell below **once** to install the required dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc8076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Google Colab, uncomment the next line to install the dependencies.\n",
    "# !pip install --quiet torch torchvision tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c80272",
   "metadata": {},
   "source": [
    "## 2. Imports and utility helpers\n",
    "\n",
    "This cell imports the libraries needed for both PyTorch and TensorFlow workflows and defines a helper for visualizing predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27303c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "def show_images(images: np.ndarray, labels: List[int], predictions: List[int] | None = None, framework: str = \"\"):\n",
    "    \"\"\"Display a grid of Fashion-MNIST images with optional predictions.\"\"\"\n",
    "    num_images = len(images)\n",
    "    cols = 5\n",
    "    rows = math.ceil(num_images / cols)\n",
    "    plt.figure(figsize=(cols * 2.2, rows * 2.2))\n",
    "    for idx, (image, label) in enumerate(zip(images, labels)):\n",
    "        plt.subplot(rows, cols, idx + 1)\n",
    "        if image.ndim == 3 and image.shape[-1] == 1:\n",
    "            image = image.squeeze(-1)\n",
    "        plt.imshow(image, cmap=\"gray\")\n",
    "        title = CLASS_NAMES[label]\n",
    "        if predictions is not None:\n",
    "            pred_name = CLASS_NAMES[predictions[idx]]\n",
    "            title = f\"GT: {title}\n",
    "Pred: {pred_name}\"\n",
    "        if framework:\n",
    "            title = f\"{framework}\n",
    "\" + title\n",
    "        plt.title(title, fontsize=9)\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34bcbc",
   "metadata": {},
   "source": [
    "## 3. PyTorch workflow\n",
    "\n",
    "We start with PyTorch and TorchVision to create a lightweight CNN classifier for Fashion-MNIST. To keep the runtime short, we train on a subset of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb35556",
   "metadata": {},
   "source": [
    "### 3.1 Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "full_train = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Use a subset (5,000 samples) for faster training in demos.\n",
    "subset_size = 5_000\n",
    "train_subset, _ = random_split(full_train, [subset_size, len(full_train) - subset_size])\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Peek at a few sample images.\n",
    "sample_images, sample_labels = next(iter(train_loader))\n",
    "show_images(sample_images[:10].numpy().transpose(0, 2, 3, 1), sample_labels[:10].tolist(), framework=\"PyTorch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75985020",
   "metadata": {},
   "source": [
    "### 3.2 Define a compact CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b39aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, len(CLASS_NAMES)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FashionCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa2d54",
   "metadata": {},
   "source": [
    "### 3.3 Train the PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d17fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_torch_model(model, data_loader, criterion, optimizer, epochs=3):\n",
    "    history = []\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        total_samples = 0\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_correct / total_samples\n",
    "        history.append((epoch_loss, epoch_acc))\n",
    "        print(f\"Epoch {epoch}: loss={epoch_loss:.4f}, accuracy={epoch_acc:.4f}\")\n",
    "    return history\n",
    "\n",
    "\n",
    "torch_history = train_torch_model(model, train_loader, criterion, optimizer, epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d5811",
   "metadata": {},
   "source": [
    "### 3.4 Evaluate the PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec23da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_images = []\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        if len(all_images) < 10:\n",
    "            all_images.extend(images.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Visualize predictions for the first batch collected.\n",
    "if all_images:\n",
    "    images_to_show = np.array(all_images[:10]).transpose(0, 2, 3, 1)\n",
    "    show_images(images_to_show, all_labels[:10], all_preds[:10], framework=\"PyTorch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ad62f",
   "metadata": {},
   "source": [
    "## 4. TensorFlow/Keras workflow\n",
    "\n",
    "We now replicate a similar experiment using TensorFlow and Keras. The data pipeline uses the same Fashion-MNIST dataset to keep comparisons fair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40802e38",
   "metadata": {},
   "source": [
    "### 4.1 Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f747bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(tf_train_images, tf_train_labels), (tf_test_images, tf_test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Normalize to [0, 1] and add channel dimension for CNN compatibility.\n",
    "tf_train_images = tf_train_images.astype(\"float32\") / 255.0\n",
    "tf_test_images = tf_test_images.astype(\"float32\") / 255.0\n",
    "tf_train_images = tf_train_images[..., np.newaxis]\n",
    "tf_test_images = tf_test_images[..., np.newaxis]\n",
    "\n",
    "# Use a subset for faster demos, matching the PyTorch sample size.\n",
    "subset_size = 5_000\n",
    "tf_train_images = tf_train_images[:subset_size]\n",
    "tf_train_labels = tf_train_labels[:subset_size]\n",
    "\n",
    "show_images(tf_train_images[:10], tf_train_labels[:10].tolist(), framework=\"TensorFlow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8567af",
   "metadata": {},
   "source": [
    "### 4.2 Build the TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d2a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(CLASS_NAMES), activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "tf_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "tf_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03084e40",
   "metadata": {},
   "source": [
    "### 4.3 Train the TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea37b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_history = tf_model.fit(\n",
    "    tf_train_images,\n",
    "    tf_train_labels,\n",
    "    validation_split=0.1,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    verbose=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c245a69",
   "metadata": {},
   "source": [
    "### 4.4 Evaluate the TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700ca72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_test_loss, tf_test_accuracy = tf_model.evaluate(tf_test_images, tf_test_labels, verbose=0)\n",
    "print(f\"Test accuracy: {tf_test_accuracy:.4f}\")\n",
    "\n",
    "tf_preds = np.argmax(tf_model.predict(tf_test_images[:10]), axis=1)\n",
    "show_images(tf_test_images[:10], tf_test_labels[:10].tolist(), tf_preds.tolist(), framework=\"TensorFlow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac4b9af",
   "metadata": {},
   "source": [
    "## 5. Reflection prompts\n",
    "\n",
    "To encourage critical thinking, consider the following questions with your class:\n",
    "\n",
    "- How do the training logs differ between PyTorch and TensorFlow?\n",
    "- What is the impact of training on a subset versus the full dataset?\n",
    "- Which model performed better on the test set? Why might that be the case?\n",
    "- How would you adapt these pipelines for transfer learning on a different image dataset?\n",
    "\n",
    "Happy teaching!\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
