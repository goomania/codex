{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8fc62a7c",
      "metadata": {},
      "source": [
        "# Computer Vision Mini Exercises\n",
        "\n",
        "Welcome to this mini set of computer vision exercises! The notebook is designed for quick demonstrations in a classroom setting and focuses on **image classification** using both **PyTorch** and **TensorFlow**.\n",
        "\n",
        "You will:\n",
        "\n",
        "1. Install the required libraries (PyTorch, TorchVision, and TensorFlow).\n",
        "2. Explore the Fashion-MNIST dataset.\n",
        "3. Train and evaluate a compact convolutional neural network (CNN) in PyTorch.\n",
        "4. Repeat a similar experiment with TensorFlow/Keras.\n",
        "5. Compare the predictions from both frameworks.\n",
        "\n",
        "> **Tip for Colab:** Runtime \u2192 Change runtime type \u2192 Hardware accelerator \u2192 GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b432b3",
      "metadata": {},
      "source": [
        "## 1. Environment setup\n",
        "\n",
        "This mini-lab is optimized for Google Colab. If you are running it there, execute the next code cell **once** to install PyTorch, TorchVision, and TensorFlow. The install step can take a couple of minutes; after it finishes Colab might prompt you to restart the runtime so that the fresh packages are picked up.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fc8076d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If running on Google Colab, uncomment the next line to install the dependencies.\n",
        "# !pip install --quiet torch torchvision tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0c80272",
      "metadata": {},
      "source": [
        "## 2. Imports and utility helpers\n",
        "\n",
        "We pull in the core scientific Python stack, both deep-learning frameworks, and a small `show_images` helper. The helper simply arranges Fashion-MNIST samples in a grid so you can quickly compare model predictions with the ground-truth labels during demos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e27303c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import List\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "CLASS_NAMES = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "def show_images(images: np.ndarray, labels: List[int], predictions: List[int] | None = None, framework: str = \"\"):\n",
        "    \"\"\"Display a grid of Fashion-MNIST images with optional predictions.\"\"\"\n",
        "    num_images = len(images)\n",
        "    cols = 5\n",
        "    rows = math.ceil(num_images / cols)\n",
        "    plt.figure(figsize=(cols * 2.2, rows * 2.2))\n",
        "    for idx, (image, label) in enumerate(zip(images, labels)):\n",
        "        plt.subplot(rows, cols, idx + 1)\n",
        "        if image.ndim == 3 and image.shape[-1] == 1:\n",
        "            image = image.squeeze(-1)\n",
        "        plt.imshow(image, cmap=\"gray\")\n",
        "        title = CLASS_NAMES[label]\n",
        "        if predictions is not None:\n",
        "            pred_name = CLASS_NAMES[predictions[idx]]\n",
        "            title = f\"GT: {title}\n",
        "Pred: {pred_name}\"\n",
        "        if framework:\n",
        "            title = f\"{framework}\n",
        "\" + title\n",
        "        plt.title(title, fontsize=9)\n",
        "        plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c34bcbc",
      "metadata": {},
      "source": [
        "## 3. PyTorch workflow\n",
        "\n",
        "The PyTorch portion walks through the traditional computer-vision loop: preparing data, defining a convolutional neural network (CNN), training the model, and reflecting on evaluation metrics. Each step mirrors what your students will see in production-scale projects, just with a smaller dataset so it runs quickly in class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeb35556",
      "metadata": {},
      "source": [
        "### 3.1 Load the dataset\n",
        "\n",
        "Fashion-MNIST provides 28\u00d728 grayscale images of clothing categories. We normalize the tensors to roughly zero-mean/\u00b11 variance so gradient descent behaves well, and we cap the training subset at 5,000 examples to keep the runtime under a minute on free Colab GPUs/CPUs. A quick visualization helps students connect pixel values to the semantic labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8dd1158",
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "])\n",
        "\n",
        "full_train = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Use a subset (5,000 samples) for faster training in demos.\n",
        "subset_size = 5_000\n",
        "train_subset, _ = random_split(full_train, [subset_size, len(full_train) - subset_size])\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Peek at a few sample images.\n",
        "sample_images, sample_labels = next(iter(train_loader))\n",
        "show_images(sample_images[:10].numpy().transpose(0, 2, 3, 1), sample_labels[:10].tolist(), framework=\"PyTorch\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75985020",
      "metadata": {},
      "source": [
        "### 3.2 Define a compact CNN\n",
        "\n",
        "This lightweight CNN stacks two convolution + pooling blocks followed by a fully-connected classifier. Highlight to students how:\n",
        "\n",
        "* Small 3\u00d73 kernels act like edge and texture detectors.\n",
        "* Max pooling halves the spatial resolution, forcing the network to focus on the most salient activations.\n",
        "* Dropout in the dense layer mitigates overfitting even on this tiny dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7b39aba",
      "metadata": {},
      "outputs": [],
      "source": [
        "class FashionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.25),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, len(CLASS_NAMES)),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FashionCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0fa2d54",
      "metadata": {},
      "source": [
        "### 3.3 Train the PyTorch model\n",
        "\n",
        "We loop over the training loader for three epochs and track loss/accuracy per epoch. Emphasize that the optimizer step follows the familiar pattern: forward pass \u2192 loss \u2192 backward pass \u2192 parameter update. The training history recorded here will be plotted right after to show convergence visually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d17fdd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_torch_model(model, data_loader, criterion, optimizer, epochs=3):\n",
        "    history = []\n",
        "    model.train()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        running_loss = 0.0\n",
        "        running_correct = 0\n",
        "        total_samples = 0\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            running_correct += (preds == labels).sum().item()\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / total_samples\n",
        "        epoch_acc = running_correct / total_samples\n",
        "        history.append((epoch_loss, epoch_acc))\n",
        "        print(f\"Epoch {epoch}: loss={epoch_loss:.4f}, accuracy={epoch_acc:.4f}\")\n",
        "    return history\n",
        "\n",
        "\n",
        "torch_history = train_torch_model(model, train_loader, criterion, optimizer, epochs=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9454b4e5",
      "metadata": {},
      "source": [
        "#### Visualize PyTorch training progress\n",
        "\n",
        "Plotting the epoch-wise loss and accuracy helps learners see whether the model is still improving or has plateaued. Invite them to describe what a diverging validation curve would look like and why it signals overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "d626a622",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "torch_losses, torch_accs = zip(*torch_history)\n",
        "epochs = range(1, len(torch_losses) + 1)\n",
        "\n",
        "fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(10, 3))\n",
        "ax_loss.plot(epochs, torch_losses, marker='o', color='#1f77b4')\n",
        "ax_loss.set_title('PyTorch loss')\n",
        "ax_loss.set_xlabel('Epoch')\n",
        "ax_loss.set_ylabel('Cross-entropy')\n",
        "ax_loss.grid(alpha=0.3)\n",
        "\n",
        "ax_acc.plot(epochs, torch_accs, marker='o', color='#ff7f0e')\n",
        "ax_acc.set_title('PyTorch accuracy')\n",
        "ax_acc.set_xlabel('Epoch')\n",
        "ax_acc.set_ylabel('Accuracy')\n",
        "ax_acc.set_ylim(0, 1)\n",
        "ax_acc.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b90d5811",
      "metadata": {},
      "source": [
        "### 3.4 Evaluate the PyTorch model\n",
        "\n",
        "During evaluation we disable gradient tracking and compute:\n",
        "\n",
        "* Overall test accuracy for a single headline number.\n",
        "* A per-class breakdown via a confusion matrix so students can spot classes that lag.\n",
        "* Example predictions (including misclassifications) to connect numbers back to images.\n",
        "\n",
        "Encourage learners to reason **why** certain items are harder\u2014for example, shirts vs. T-shirts look visually similar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26ec23da",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "confusion_matrix = torch.zeros(len(CLASS_NAMES), len(CLASS_NAMES), dtype=torch.int64)\n",
        "all_images = []\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "misclassified_images = []\n",
        "misclassified_labels = []\n",
        "misclassified_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        for true_label, pred_label in zip(labels.view(-1), preds.view(-1)):\n",
        "            confusion_matrix[int(true_label), int(pred_label)] += 1\n",
        "\n",
        "        if len(all_images) < 10:\n",
        "            all_images.extend(images.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "        if len(misclassified_images) < 6:\n",
        "            mismatch = (preds != labels).nonzero(as_tuple=False).flatten()\n",
        "            for idx in mismatch:\n",
        "                if len(misclassified_images) >= 6:\n",
        "                    break\n",
        "                misclassified_images.append(images[idx].cpu().numpy())\n",
        "                misclassified_labels.append(int(labels[idx].cpu()))\n",
        "                misclassified_preds.append(int(preds[idx].cpu()))\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "per_class_accuracy = confusion_matrix.diag().float() / confusion_matrix.sum(dim=1).clamp(min=1)\n",
        "print(\"Per-class accuracy (PyTorch):\")\n",
        "for class_name, acc, hits, total_count in zip(\n",
        "    CLASS_NAMES,\n",
        "    per_class_accuracy.tolist(),\n",
        "    confusion_matrix.diag().tolist(),\n",
        "    confusion_matrix.sum(dim=1).tolist(),\n",
        "):\n",
        "    print(f\"  {class_name:>12}: {acc:.3f} ({int(hits)}/{int(total_count)})\")\n",
        "\n",
        "if all_images:\n",
        "    images_to_show = np.array(all_images[:10]).transpose(0, 2, 3, 1)\n",
        "    show_images(images_to_show, all_labels[:10], all_preds[:10], framework=\"PyTorch\")\n",
        "\n",
        "if misclassified_images:\n",
        "    mis_to_show = np.array(misclassified_images).transpose(0, 2, 3, 1)\n",
        "    show_images(\n",
        "        mis_to_show,\n",
        "        misclassified_labels,\n",
        "        misclassified_preds,\n",
        "        framework=\"PyTorch misclassified\",\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1fda3e3",
      "metadata": {},
      "source": [
        "#### Interpreting the PyTorch results\n",
        "\n",
        "* The headline accuracy provides a quick health check\u2014aim for ~85\u201390% within three epochs on this subset.\n",
        "* Scan the per-class table for imbalances. Lower scores on `Shirt` or `Coat` usually stem from overlapping silhouettes.\n",
        "* Review the misclassified image grid and ask: *What visual cues misled the network?* Encourage students to articulate hypotheses (e.g., low contrast, cropped hems) and propose data or architecture tweaks to address them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f84ad62f",
      "metadata": {},
      "source": [
        "## 4. TensorFlow/Keras workflow\n",
        "\n",
        "The TensorFlow section mirrors the PyTorch steps with Keras' high-level APIs. This gives students a point-by-point comparison between imperative and declarative training styles while keeping the underlying computer-vision principles identical.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40802e38",
      "metadata": {},
      "source": [
        "### 4.1 Load and preprocess data\n",
        "\n",
        "Keras ships Fashion-MNIST directly. We scale pixel intensities to [0, 1], add a channel dimension so convolution layers accept the input, and again trim the training set to 5,000 examples. Showing the same preview grid reinforces that the two workflows operate on identical data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f747bdb",
      "metadata": {},
      "outputs": [],
      "source": [
        "(tf_train_images, tf_train_labels), (tf_test_images, tf_test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Normalize to [0, 1] and add channel dimension for CNN compatibility.\n",
        "tf_train_images = tf_train_images.astype(\"float32\") / 255.0\n",
        "tf_test_images = tf_test_images.astype(\"float32\") / 255.0\n",
        "tf_train_images = tf_train_images[..., np.newaxis]\n",
        "tf_test_images = tf_test_images[..., np.newaxis]\n",
        "\n",
        "# Use a subset for faster demos, matching the PyTorch sample size.\n",
        "subset_size = 5_000\n",
        "tf_train_images = tf_train_images[:subset_size]\n",
        "tf_train_labels = tf_train_labels[:subset_size]\n",
        "\n",
        "show_images(tf_train_images[:10], tf_train_labels[:10].tolist(), framework=\"TensorFlow\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8567af",
      "metadata": {},
      "source": [
        "### 4.2 Build the TensorFlow model\n",
        "\n",
        "The architecture intentionally mirrors the PyTorch CNN: convolution/pooling pairs followed by a dropout-regularized dense classifier. Point out how Keras layers encapsulate both the parameters and the forward pass, whereas in PyTorch we defined them manually inside `forward`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39d2a6de",
      "metadata": {},
      "outputs": [],
      "source": [
        "tf_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(len(CLASS_NAMES), activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "tf_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "tf_model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03084e40",
      "metadata": {},
      "source": [
        "### 4.3 Train the TensorFlow model\n",
        "\n",
        "`model.fit` handles the epoch loop, but the underlying steps are the same. We also reserve 10% of the data for validation so you can compare training vs. validation curves and discuss under/overfitting signals.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea37b78",
      "metadata": {},
      "outputs": [],
      "source": [
        "tf_history = tf_model.fit(\n",
        "    tf_train_images,\n",
        "    tf_train_labels,\n",
        "    validation_split=0.1,\n",
        "    epochs=3,\n",
        "    batch_size=64,\n",
        "    verbose=2,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfa0a662",
      "metadata": {},
      "source": [
        "#### Visualize TensorFlow training progress\n",
        "\n",
        "Keras returns a `History` object with tracked metrics. Plotting them side by side with the PyTorch curves makes it easy to compare optimization dynamics across frameworks.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "4a252137",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "tf_epochs = range(1, len(tf_history.history['loss']) + 1)\n",
        "\n",
        "fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(10, 3))\n",
        "ax_loss.plot(tf_epochs, tf_history.history['loss'], marker='o', label='train', color='#1f77b4')\n",
        "ax_loss.plot(tf_epochs, tf_history.history['val_loss'], marker='o', label='val', color='#9467bd')\n",
        "ax_loss.set_title('TensorFlow loss')\n",
        "ax_loss.set_xlabel('Epoch')\n",
        "ax_loss.set_ylabel('Cross-entropy')\n",
        "ax_loss.grid(alpha=0.3)\n",
        "ax_loss.legend()\n",
        "\n",
        "ax_acc.plot(tf_epochs, tf_history.history['accuracy'], marker='o', label='train', color='#ff7f0e')\n",
        "ax_acc.plot(tf_epochs, tf_history.history['val_accuracy'], marker='o', label='val', color='#2ca02c')\n",
        "ax_acc.set_title('TensorFlow accuracy')\n",
        "ax_acc.set_xlabel('Epoch')\n",
        "ax_acc.set_ylabel('Accuracy')\n",
        "ax_acc.set_ylim(0, 1)\n",
        "ax_acc.grid(alpha=0.3)\n",
        "ax_acc.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c245a69",
      "metadata": {},
      "source": [
        "### 4.4 Evaluate the TensorFlow model\n",
        "\n",
        "After training we compute the same accuracy and confusion-matrix diagnostics, then inspect predictions and misclassifications. Ask learners to compare the TensorFlow and PyTorch outputs\u2014do they stumble on the same categories, or do the results differ because of weight initialization and optimizer nuances?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "700ca72b",
      "metadata": {},
      "outputs": [],
      "source": [
        "tf_test_loss, tf_test_accuracy = tf_model.evaluate(tf_test_images, tf_test_labels, verbose=0)\n",
        "print(f\"Test accuracy: {tf_test_accuracy:.4f}\")\n",
        "\n",
        "tf_probs = tf_model.predict(tf_test_images, batch_size=256, verbose=0)\n",
        "tf_preds = tf_probs.argmax(axis=1)\n",
        "\n",
        "confusion_matrix = tf.math.confusion_matrix(tf_test_labels, tf_preds, num_classes=len(CLASS_NAMES)).numpy()\n",
        "per_class_accuracy = confusion_matrix.diagonal() / np.maximum(confusion_matrix.sum(axis=1), 1)\n",
        "\n",
        "print(\"Per-class accuracy (TensorFlow):\")\n",
        "for class_name, acc, hits, total_count in zip(\n",
        "    CLASS_NAMES,\n",
        "    per_class_accuracy.tolist(),\n",
        "    confusion_matrix.diagonal().tolist(),\n",
        "    confusion_matrix.sum(axis=1).tolist(),\n",
        "):\n",
        "    print(f\"  {class_name:>12}: {acc:.3f} ({int(hits)}/{int(total_count)})\")\n",
        "\n",
        "show_images(tf_test_images[:10], tf_test_labels[:10].tolist(), tf_preds[:10].tolist(), framework=\"TensorFlow\")\n",
        "\n",
        "mis_idx = np.where(tf_preds != tf_test_labels)[0][:6]\n",
        "if mis_idx.size:\n",
        "    show_images(\n",
        "        tf_test_images[mis_idx],\n",
        "        tf_test_labels[mis_idx].tolist(),\n",
        "        tf_preds[mis_idx].tolist(),\n",
        "        framework=\"TensorFlow misclassified\",\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3348e88",
      "metadata": {},
      "source": [
        "#### Interpreting the TensorFlow results\n",
        "\n",
        "* Compare these metrics with the PyTorch run\u2014close agreement suggests both implementations learned similar decision boundaries.\n",
        "* Validation-vs-training curves can reveal early signs of overfitting; if validation accuracy plateaus sooner, discuss early stopping or stronger regularization.\n",
        "* Misclassified examples create space for qualitative analysis. Ask students to annotate what feature (texture, shape, context) would help the network fix the prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ac4b9af",
      "metadata": {},
      "source": [
        "## 5. Reflection prompts\n",
        "\n",
        "* Compare the training curves from PyTorch and TensorFlow. Which framework converged faster? What hyperparameters might explain the difference?\n",
        "* Look at the per-class accuracies. Which categories are consistently strong or weak across both models? How could you collect or augment data to close the gap?\n",
        "* How would you modify the architecture if you needed higher accuracy without drastically increasing training time on Colab?\n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}
