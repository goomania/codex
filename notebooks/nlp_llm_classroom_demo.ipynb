{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6adc048",
   "metadata": {},
   "source": [
    "# NLP LLM Classroom Demo\n",
    "\n",
    "This notebook demonstrates a compact workflow for teaching natural language processing tasks with pretrained large language models (LLMs) from [Hugging Face](https://huggingface.co/models). Each section contains:\n",
    "\n",
    "1. A short explanation of the task.\n",
    "2. A runnable code cell using a ready-to-use model.\n",
    "3. Discussion of the output and classroom talking points.\n",
    "\n",
    "> **Note:** All examples rely on freely available, relatively small models so that they run quickly on Google Colab's default CPU runtime. Replace them with larger models for higher quality when more resources are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a2c94",
   "metadata": {},
   "source": [
    "## 0. Environment Setup\n",
    "\n",
    "Only two lightweight libraries are needed. `transformers` gives us the model pipelines and `sentencepiece` is required by a few multilingual models. The installation takes ~1 minute in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe780222",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837ae4e",
   "metadata": {},
   "source": [
    "## 1. Imports and Helper Utilities\n",
    "\n",
    "The `pipeline` helper from ðŸ¤— Transformers abstracts away tokenization, model loading, and post-processing. The helper function `run_pipeline` standardizes how we print model outputs for the classroom demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def run_pipeline(task_pipeline, *args, **kwargs):\n",
    "    '''Execute a pipeline call and pretty-print the result.'''\n",
    "    result = task_pipeline(*args, **kwargs)\n",
    "    if isinstance(result, list):\n",
    "        for idx, item in enumerate(result, start=1):\n",
    "            print(f\"Result {idx}:\")\n",
    "            if isinstance(item, dict):\n",
    "                for key, value in item.items():\n",
    "                    print(f\"  {key}: {value}\")\n",
    "            else:\n",
    "                print(item)\n",
    "            print()\n",
    "    elif isinstance(result, dict):\n",
    "        for key, value in result.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f92055",
   "metadata": {},
   "source": [
    "## 2. Sentiment Analysis\n",
    "\n",
    "**Goal:** Classify the emotional tone (positive/negative) of a text snippet.\n",
    "\n",
    "**Model:** `distilbert-base-uncased-finetuned-sst-2-english` (fine-tuned on the Stanford Sentiment Treebank)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ecd0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\n",
    "    task=\"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "sample_reviews = [\n",
    "    \"The new course content is incredibly engaging!\",\n",
    "    \"I struggled to understand the instructions and felt lost.\"\n",
    "]\n",
    "\n",
    "sentiment_results = run_pipeline(sentiment_pipeline, sample_reviews)\n",
    "sentiment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9d0e7",
   "metadata": {},
   "source": [
    "### Teaching Notes\n",
    "- Highlight how the model returns both the predicted label and confidence score (`score`).\n",
    "- Encourage students to think about class imbalance and what kinds of text might be ambiguous for the classifier.\n",
    "- Prompt a discussion about expanding from binary to multi-class sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed46945",
   "metadata": {},
   "source": [
    "## 3. Named Entity Recognition (NER)\n",
    "\n",
    "**Goal:** Detect and categorize real-world entities (people, organizations, locations, etc.) mentioned in text.\n",
    "\n",
    "**Model:** `dslim/bert-base-NER`, a multilingual BERT fine-tuned for token-level entity tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3311e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipeline(\n",
    "    task=\"ner\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "ner_text = (\n",
    "    \"Hugging Face was founded in New York and now has offices in Paris and San Francisco.\"\n",
    ")\n",
    "\n",
    "ner_results = run_pipeline(ner_pipeline, ner_text)\n",
    "ner_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba14726",
   "metadata": {},
   "source": [
    "### Teaching Notes\n",
    "- Point out how entity spans are grouped with the `aggregation_strategy`. Without it, students would see token-level predictions.\n",
    "- Ask learners to consider how the model might struggle with less common entity types or languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cfeb31",
   "metadata": {},
   "source": [
    "## 4. Extractive Question Answering\n",
    "\n",
    "**Goal:** Given a question and a reference passage, extract the span of text that best answers the question.\n",
    "\n",
    "**Model:** `distilbert-base-uncased-distilled-squad`, fine-tuned on the SQuAD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bf4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = pipeline(\n",
    "    task=\"question-answering\",\n",
    "    model=\"distilbert-base-uncased-distilled-squad\"\n",
    ")\n",
    "\n",
    "qa_context = (\n",
    "    \"Large language models are pretrained on enormous text corpora. \"\n",
    "    \"Fine-tuning them on task-specific data can drastically improve accuracy.\"\n",
    ")\n",
    "\n",
    "qa_prompt = {\n",
    "    \"question\": \"Why do we fine-tune pretrained language models?\",\n",
    "    \"context\": qa_context\n",
    "}\n",
    "\n",
    "qa_result = run_pipeline(qa_pipeline, qa_prompt)\n",
    "qa_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd47b0a",
   "metadata": {},
   "source": [
    "### Teaching Notes\n",
    "- Emphasize the difference between extractive QA (selecting text) and generative QA (producing text from scratch).\n",
    "- Discuss scenarios where the answer is not present in the context and how the model might respond."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de47451b",
   "metadata": {},
   "source": [
    "## 5. Abstractive Summarization\n",
    "\n",
    "**Goal:** Compress long-form text into a concise summary that captures the main ideas.\n",
    "\n",
    "**Model:** `sshleifer/distilbart-cnn-12-6`, a distilled version of BART fine-tuned on news articles. It is fast enough for live demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_pipeline = pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=\"sshleifer/distilbart-cnn-12-6\"\n",
    ")\n",
    "\n",
    "long_text = (\n",
    "    \"Transformers have transformed NLP by enabling models to learn contextual relationships \"\n",
    "    \"in text using self-attention. They underpin state-of-the-art systems for translation, \"\n",
    "    \"summarization, and question answering, and are now being scaled into multi-billion \"\n",
    "    \"parameter large language models deployed in production.\"\n",
    ")\n",
    "\n",
    "summary = run_pipeline(\n",
    "    summarization_pipeline,\n",
    "    long_text,\n",
    "    max_length=60,\n",
    "    min_length=25,\n",
    "    do_sample=False\n",
    ")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e240552",
   "metadata": {},
   "source": [
    "### Teaching Notes\n",
    "- Ask students to compare the generated summary with the original paragraph.\n",
    "- Talk about length control via `max_length` and `min_length` and why deterministic decoding (`do_sample=False`) is helpful for reproducible demos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ec7ce",
   "metadata": {},
   "source": [
    "## 6. Generative Text Completion (GPT-style)\n",
    "\n",
    "**Goal:** Produce fluent continuations of a prompt using an auto-regressive language model.\n",
    "\n",
    "**Model:** `distilgpt2`, a compact GPT-2 variant suitable for CPU demos. This illustrates the workflow behind larger GPT-family models (including future versions such as GPT-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac35e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_pipeline = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"distilgpt2\",\n",
    "    pad_token_id=50256  # ensure the pipeline handles end-of-text token correctly\n",
    ")\n",
    "\n",
    "generation_prompt = \"In the future of AI education, teachers will\"\n",
    "\n",
    "completion = run_pipeline(\n",
    "    generation_pipeline,\n",
    "    generation_prompt,\n",
    "    max_length=80,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7\n",
    ")\n",
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4dc044",
   "metadata": {},
   "source": [
    "### Teaching Notes\n",
    "- Explain sampling parameters like `temperature` and `num_return_sequences`.\n",
    "- Encourage experimentation with different prompts to explore model creativity and limitations.\n",
    "- Connect this demo to production-scale GPT workflows (e.g., GPT-4 or future GPT-5) that follow the same API pattern but require managed inference endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917c08c",
   "metadata": {},
   "source": [
    "## 7. (Optional) Connecting to Hosted LLMs\n",
    "\n",
    "If you have access to managed endpoints (e.g., Hugging Face Inference Endpoints or future GPT-5 APIs), you can adapt the same workflow:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "hf_hub_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    token=\"YOUR_HF_TOKEN\",\n",
    "    revision=\"main\"\n",
    ")\n",
    "print(hf_hub_pipeline(\"Explain the role of attention in transformers.\"))\n",
    "```\n",
    "\n",
    "For OpenAI-style GPT endpoints, the logic is similarâ€”submit a prompt and receive generated textâ€”but requires the `openai` (or `openai-python`) package and an API key. Keeping this optional avoids extra dependencies for the classroom session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee50d7b",
   "metadata": {},
   "source": [
    "## 8. Cleanup Tip\n",
    "\n",
    "Models downloaded during the session are cached under `~/.cache/huggingface`. In shared environments, you can clear them after class with:\n",
    "\n",
    "```python\n",
    "!rm -rf ~/.cache/huggingface\n",
    "```\n",
    "\n",
    "This step is optional but keeps storage usage low on repeated runs."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
