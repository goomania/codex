{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f3be2b6",
   "metadata": {},
   "source": [
    "# Hands-on ELT with dbt and DuckDB\n",
    "\n",
    "This notebook walks through a complete **extract-load-transform (ELT)** workflow using [dbt](https://www.getdbt.com/) on a demo dataset. It is designed for teaching data engineering concepts with a focus on how analytics engineers can build reliable transformations on top of an existing warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c08449",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "\n",
    "By the end of this lab you will be able to:\n",
    "\n",
    "* Explain the difference between ETL and ELT and why dbt aligns with modern ELT practices.\n",
    "* Create a minimal dbt project that targets DuckDB as the analytical warehouse.\n",
    "* Seed raw data, build staged models, construct a mart, and add data quality tests.\n",
    "* Use dbt to run transformations and validate results from a Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1888ec",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Python 3.9+ with `pip`.\n",
    "* Jupyter environment with access to install packages.\n",
    "* Basic familiarity with SQL and the command line.\n",
    "\n",
    "> **Tip:** Every code cell in this notebook is safe to re-run. If you re-run the notebook, existing directories and files will be reused."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056de74",
   "metadata": {},
   "source": [
    "## 1. Install dependencies\n",
    "\n",
    "We will use the [`dbt-duckdb`](https://github.com/dbt-labs/dbt-duckdb) adapter so that dbt can run SQL transformations locally without needing an external database server.\n",
    "\n",
    "The installation may take a minute the first time it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75382475",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet dbt-duckdb duckdb pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44faf58",
   "metadata": {},
   "source": [
    "## 2. Set up the project structure\n",
    "\n",
    "We will create a dedicated folder called `dbt_elt_demo` inside the current working directory.\n",
    "\n",
    "* `DBT_PROFILES_DIR` is set so dbt can discover the profile configuration that tells it how to connect to DuckDB.\n",
    "* `project_root` will be reused throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d7d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd() / \"dbt_elt_demo\"\n",
    "project_root.mkdir(exist_ok=True)\n",
    "os.environ[\"DBT_PROFILES_DIR\"] = str(project_root / \"profiles\")\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"DBT_PROFILES_DIR:\", os.environ[\"DBT_PROFILES_DIR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d7d3c8",
   "metadata": {},
   "source": [
    "Create the sub-directories that dbt expects for models, seeds, and profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66727a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_staging = project_root / \"models\" / \"staging\"\n",
    "models_marts = project_root / \"models\" / \"marts\"\n",
    "seeds_dir = project_root / \"seeds\"\n",
    "profiles_dir = Path(os.environ[\"DBT_PROFILES_DIR\"])\n",
    "\n",
    "for path in [models_staging, models_marts, seeds_dir, profiles_dir]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "project_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a454eabb",
   "metadata": {},
   "source": [
    "## 3. Create a demo dataset\n",
    "\n",
    "We will simulate data from an online subscription business.\n",
    "\n",
    "* `customers` contains customer attributes.\n",
    "* `orders` records transactions.\n",
    "\n",
    "In a real ELT process, data would already be loaded into the warehouse before dbt runs. Here we treat the CSV files as our \"loaded\" layer by storing them in dbt's `seeds` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "customers = pd.DataFrame(\n",
    "    {\n",
    "        \"customer_id\": [1, 2, 3, 4],\n",
    "        \"customer_name\": [\"Avery Analytics\", \"Bobby Business\", \"Casey Commerce\", \"Dakota Data\"],\n",
    "        \"customer_email\": [\n",
    "            \"avery.analytics@example.com\",\n",
    "            \"bobby.business@example.com\",\n",
    "            \"casey.commerce@example.com\",\n",
    "            \"dakota.data@example.com\",\n",
    "        ],\n",
    "        \"customer_segment\": [\"SMB\", \"Enterprise\", \"Enterprise\", \"SMB\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "orders = pd.DataFrame(\n",
    "    {\n",
    "        \"order_id\": [101, 102, 103, 104, 105, 106],\n",
    "        \"customer_id\": [1, 2, 3, 2, 4, 1],\n",
    "        \"order_date\": pd.to_datetime([\n",
    "            \"2024-01-15\",\n",
    "            \"2024-01-17\",\n",
    "            \"2024-01-18\",\n",
    "            \"2024-02-03\",\n",
    "            \"2024-02-10\",\n",
    "            \"2024-03-05\",\n",
    "        ]),\n",
    "        \"status\": [\"paid\", \"paid\", \"pending\", \"paid\", \"paid\", \"refunded\"],\n",
    "        \"amount_usd\": [1200.0, 2400.0, 800.0, 2600.0, 900.0, 1200.0],\n",
    "    }\n",
    ")\n",
    "\n",
    "customers.to_csv(seeds_dir / \"customers.csv\", index=False)\n",
    "orders.to_csv(seeds_dir / \"orders.csv\", index=False)\n",
    "\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956ce11",
   "metadata": {},
   "source": [
    "Preview the `orders` dataset to understand what transformations we might want to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad9cced",
   "metadata": {},
   "source": [
    "## 4. Configure dbt\n",
    "\n",
    "dbt needs two YAML files:\n",
    "\n",
    "1. `dbt_project.yml` – defines project-level settings like paths and materializations.\n",
    "2. `profiles.yml` – stored in the profiles directory and describes how to connect to the warehouse (DuckDB in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c932bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_project_yaml = \"\"\"\n",
    "name: 'dbt_elt_demo'\n",
    "version: '1.0.0'\n",
    "config-version: 2\n",
    "\n",
    "profile: 'dbt_elt_demo'\n",
    "\n",
    "model-paths: ['models']\n",
    "seed-paths: ['seeds']\n",
    "\n",
    "models:\n",
    "  dbt_elt_demo:\n",
    "    staging:\n",
    "      +materialized: view\n",
    "    marts:\n",
    "      +materialized: table\n",
    "\"\"\"\n",
    "\n",
    "(project_root / \"dbt_project.yml\").write_text(dbt_project_yaml.strip() + \"\n",
    "\")\n",
    "print((project_root / \"dbt_project.yml\").read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5971f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_yaml = f\"\"\"\n",
    "dbt_elt_demo:\n",
    "  target: dev\n",
    "  outputs:\n",
    "    dev:\n",
    "      type: duckdb\n",
    "      path: '{project_root / \"warehouse.duckdb\"}'\n",
    "      schema: main\n",
    "      threads: 4\n",
    "\"\"\"\n",
    "\n",
    "(profiles_dir / \"profiles.yml\").write_text(profiles_yaml.strip() + \"\n",
    "\")\n",
    "print((profiles_dir / \"profiles.yml\").read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16c75f2",
   "metadata": {},
   "source": [
    "## 5. Build the staging layer\n",
    "\n",
    "Staging models clean and standardize raw data. They are thin wrappers over the seeds that cast types, rename columns, or filter data.\n",
    "\n",
    "We will also declare tests in `schema.yml` to check for basic data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8068ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "stg_customers_sql = \"\"\"\n",
    "select\n",
    "    customer_id,\n",
    "    customer_name,\n",
    "    customer_email,\n",
    "    customer_segment\n",
    "from {{ ref('customers') }}\n",
    "\"\"\"\n",
    "\n",
    "stg_orders_sql = \"\"\"\n",
    "select\n",
    "    order_id,\n",
    "    customer_id,\n",
    "    order_date,\n",
    "    status,\n",
    "    amount_usd\n",
    "from {{ ref('orders') }}\n",
    "\"\"\"\n",
    "\n",
    "(models_staging / \"stg_customers.sql\").write_text(stg_customers_sql.strip() + \"\n",
    "\")\n",
    "(models_staging / \"stg_orders.sql\").write_text(stg_orders_sql.strip() + \"\n",
    "\")\n",
    "\n",
    "print((models_staging / \"stg_customers.sql\").read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e659e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((models_staging / 'stg_orders.sql').read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe83ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_schema_yaml = \"\"\"\n",
    "version: 2\n",
    "\n",
    "seeds:\n",
    "  - name: customers\n",
    "    columns:\n",
    "      - name: customer_id\n",
    "        tests: [unique, not_null]\n",
    "  - name: orders\n",
    "    columns:\n",
    "      - name: order_id\n",
    "        tests: [unique, not_null]\n",
    "\n",
    "models:\n",
    "  - name: stg_customers\n",
    "    columns:\n",
    "      - name: customer_id\n",
    "        tests: [unique, not_null]\n",
    "  - name: stg_orders\n",
    "    columns:\n",
    "      - name: order_id\n",
    "        tests: [unique, not_null]\n",
    "      - name: customer_id\n",
    "        tests: [not_null]\n",
    "\"\"\"\n",
    "\n",
    "(models_staging / \"schema.yml\").write_text(staging_schema_yaml.strip() + \"\n",
    "\")\n",
    "print((models_staging / \"schema.yml\").read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf139b",
   "metadata": {},
   "source": [
    "## 6. Build the mart layer\n",
    "\n",
    "Marts aggregate business-friendly data models. Here we create a simple fact table that joins orders with their customers and adds a monthly grain.\n",
    "\n",
    "This is the type of dataset analysts would use to power dashboards and downstream metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "marts_sql = \"\"\"\n",
    "with orders as (\n",
    "    select * from {{ ref('stg_orders') }}\n",
    "),\n",
    "customers as (\n",
    "    select * from {{ ref('stg_customers') }}\n",
    ")\n",
    "\n",
    "select\n",
    "    o.order_id,\n",
    "    o.order_date,\n",
    "    o.status,\n",
    "    o.customer_id,\n",
    "    c.customer_name,\n",
    "    c.customer_segment,\n",
    "    o.amount_usd,\n",
    "    date_trunc('month', o.order_date) as order_month\n",
    "from orders o\n",
    "join customers c on o.customer_id = c.customer_id\n",
    "\"\"\"\n",
    "\n",
    "(models_marts / \"fct_customer_orders.sql\").write_text(marts_sql.strip() + \"\n",
    "\")\n",
    "print((models_marts / \"fct_customer_orders.sql\").read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b85737",
   "metadata": {},
   "source": [
    "## 7. Run dbt commands\n",
    "\n",
    "We are ready to run the project end-to-end:\n",
    "\n",
    "1. `dbt debug` confirms configuration.\n",
    "2. `dbt seed` loads the CSVs into DuckDB.\n",
    "3. `dbt run` builds models in dependency order.\n",
    "4. `dbt test` executes our data quality tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e99c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {project_root} && dbt debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {project_root} && dbt seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59797b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {project_root} && dbt run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {project_root} && dbt test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc50677",
   "metadata": {},
   "source": [
    "## 8. Explore the results\n",
    "\n",
    "The transformations were materialized inside the DuckDB database specified in our profile. Use DuckDB directly from Python to inspect the final fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5857fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "con = duckdb.connect(str(project_root / \"warehouse.duckdb\"))\n",
    "con.execute(\n",
    "    \"\"\"\n",
    "    select\n",
    "        order_month,\n",
    "        customer_segment,\n",
    "        count(*) as orders,\n",
    "        sum(amount_usd) as revenue\n",
    "    from fct_customer_orders\n",
    "    group by 1, 2\n",
    "    order by 1, 2\n",
    "    \"\"\"\n",
    ").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e027ce7",
   "metadata": {},
   "source": [
    "For a more granular view, we can display the fact table itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d243d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute('select * from fct_customer_orders order by order_date').df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82d210d",
   "metadata": {},
   "source": [
    "## 9. Clean up (optional)\n",
    "\n",
    "If you want to start fresh, remove the `dbt_elt_demo` folder and rerun the notebook. Keeping it can be useful for exploring the compiled SQL under `target/` and the generated DuckDB file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eb2e59",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "* Add slowly changing dimensions or incremental models.\n",
    "* Schedule dbt runs with a tool such as dbt Cloud or an orchestration platform.\n",
    "* Generate documentation with `dbt docs generate` and explore the lineage graph."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
