{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b137f3",
   "metadata": {},
   "source": [
    "\n",
    "# Computer Vision Classification Demo\n",
    "\n",
    "This notebook demonstrates how to build a simple image classification pipeline using both **PyTorch** and **TensorFlow**. The examples use the Fashion-MNIST dataset so that you can run everything quickly on Google Colab without installing many extra libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f578c3",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Imports and Random Seeds\n",
    "\n",
    "We start by importing the required libraries and setting seeds for reproducibility. Both frameworks are available by default in most Colab runtimes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0bb17b",
   "metadata": {},
   "source": [
    "\n",
    "## 2. PyTorch Classification Workflow\n",
    "\n",
    "We will train a lightweight convolutional neural network (CNN) on a subset of Fashion-MNIST. Using a subset keeps the runtime short while still demonstrating the end-to-end workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea6772",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1 Load and Inspect the Dataset\n",
    "\n",
    "Fashion-MNIST contains 28Ã—28 grayscale images of clothing items. `torchvision` downloads the dataset automatically and provides convenient transforms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60e8fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Use a manageable subset for faster demo runs\n",
    "train_subset = Subset(train_dataset, range(6000))\n",
    "test_subset = Subset(test_dataset, range(1000))\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=128)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "print(f'Train batches: {len(train_loader)}, Test batches: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dbc874",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 Define the CNN Model\n",
    "\n",
    "A small CNN with two convolutional layers followed by a fully connected head is sufficient for this demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731b73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "model = FashionCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa3e2dd",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 Train the Model\n",
    "\n",
    "We train the network for a few epochs. Even with only one or two epochs, the accuracy should surpass random guessing and is enough for a classroom demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8eb3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "EPOCHS = 2\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss = train(model, train_loader, optimizer, criterion)\n",
    "    print(f'Epoch {epoch}/{EPOCHS} - Training Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbceb80",
   "metadata": {},
   "source": [
    "\n",
    "### 2.4 Evaluate on the Test Set\n",
    "\n",
    "We compute the accuracy on the held-out test subset to confirm that the model learned meaningful patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3928c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "accuracy = evaluate(model, test_loader)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17408aff",
   "metadata": {},
   "source": [
    "\n",
    "### 2.5 Visualize Predictions\n",
    "\n",
    "Finally, we inspect a few predictions to make the results more tangible for students.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af6dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "images, labels = next(iter(test_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "outputs = model(images)\n",
    "preds = outputs.argmax(dim=1)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(images[i].cpu().squeeze(), cmap='gray')\n",
    "    plt.title(f'True: {class_names[labels[i]]}\n",
    "Pred: {class_names[preds[i]]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4acf2eb",
   "metadata": {},
   "source": [
    "\n",
    "## 3. TensorFlow Classification Workflow\n",
    "\n",
    "Next, we recreate the same experiment using TensorFlow/Keras. This provides a side-by-side comparison of the two most popular deep learning frameworks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706d8ce",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 Load and Prepare the Data\n",
    "\n",
    "Keras ships with the Fashion-MNIST dataset. We normalize pixel values to the `[0, 1]` range and keep a validation split for monitoring training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407cc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Keep the dataset small for quick runs\n",
    "train_limit = 6000\n",
    "test_limit = 1000\n",
    "x_train, y_train = x_train[:train_limit], y_train[:train_limit]\n",
    "x_test, y_test = x_test[:test_limit], y_test[:test_limit]\n",
    "\n",
    "# Normalize and add a channel dimension\n",
    "x_train = (x_train / 255.0)[..., np.newaxis]\n",
    "x_test = (x_test / 255.0)[..., np.newaxis]\n",
    "\n",
    "val_split = 0.1\n",
    "print(f\"Training samples: {x_train.shape[0]}, Test samples: {x_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62417ba4",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Build the Keras Model\n",
    "\n",
    "The architecture mirrors the PyTorch model: two convolutional blocks followed by dense layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a81977",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "keras_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4176981d",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3 Train the Model\n",
    "\n",
    "We train for a few epochs with a validation split so that we can monitor both training and validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0201521",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = keras_model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    validation_split=val_split,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c1d20",
   "metadata": {},
   "source": [
    "\n",
    "### 3.4 Evaluate and Visualize Predictions\n",
    "\n",
    "As with the PyTorch model, we compute the test accuracy and look at a few sample predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a874bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = keras_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test Accuracy: {test_acc * 100:.2f}%')\n",
    "\n",
    "probabilities = tf.nn.softmax(keras_model.predict(x_test[:6]))\n",
    "predictions = tf.argmax(probabilities, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(x_test[i].squeeze(), cmap='gray')\n",
    "    plt.title(f'True: {class_names[y_test[i]]}\n",
    "Pred: {class_names[predictions[i]]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a571450",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Key Takeaways\n",
    "\n",
    "* Both PyTorch and TensorFlow follow the same high-level workflow: prepare data, build a model, train, and evaluate.\n",
    "* Subclassed models (PyTorch) and Sequential models (TensorFlow) express the same architecture with minimal syntax differences.\n",
    "* Fashion-MNIST is a lightweight dataset that makes it easy to demonstrate core computer vision concepts in class.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
