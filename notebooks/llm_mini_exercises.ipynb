{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c43ec8",
   "metadata": {},
   "source": [
    "# Mini Exercises: Large Language Models with Hugging Face\n",
    "\n",
    "Welcome to a quick tour of practical activities you can use to help students experiment with open-source large language models (LLMs). This notebook is structured as a mini workshop with short, self-contained tasks that demonstrate different natural language capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797fc17",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "- Understand how to load and run Hugging Face Transformers pipelines for common LLM tasks.\n",
    "- Practice prompt engineering for short-form text generation.\n",
    "- Explore how encoder-decoder models perform abstractive summarization.\n",
    "- Experiment with zero-shot classification to tag text with arbitrary labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c4706",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "These exercises rely on the [transformers](https://huggingface.co/docs/transformers/index) library and a few lightweight models hosted on Hugging Face.\n",
    "\n",
    "Run the cell below to install any missing dependencies inside your environment. (This only needs to be done once per runtime.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on a fresh environment, uncomment the next line.\n",
    "# !pip install -q transformers datasets accelerate torch --upgrade\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55e99bb",
   "metadata": {},
   "source": [
    "You can verify your installation and the available devices (CPU/GPU) with the following optional diagnostic cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfdf5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: check library versions and available hardware\n",
    "from transformers.utils import is_torch_available\n",
    "\n",
    "if is_torch_available():\n",
    "    import torch\n",
    "    import transformers\n",
    "    print('Transformers version:', transformers.__version__)\n",
    "    print('Torch version:', torch.__version__)\n",
    "    print('Available device:', 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "    print('Transformers is not installed yet. Run the pip install cell above.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4596ce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Prompt-based text generation\n",
    "Large language models trained for causal language modeling (CLM) can generate coherent continuations of a prompt. We will use [`distilgpt2`](https://huggingface.co/distilgpt2), a lightweight distilled GPT-2 variant suitable for classroom demos.\n",
    "\n",
    "### Exercise\n",
    "1. Choose a short topic sentence to seed the model.\n",
    "2. Experiment with parameters such as `max_new_tokens`, `temperature`, and `num_return_sequences`.\n",
    "3. Compare the generated completions and discuss how sampling parameters affect creativity vs. coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8adb040",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# TODO: replace this prompt with your own idea (e.g., a course topic or debate question).\n",
    "prompt = \"In 2030, classrooms will use AI to\"\n",
    "\n",
    "try:\n",
    "    generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    outputs = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=60,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=2,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    for idx, output in enumerate(outputs, start=1):\n",
    "        print(f\"--- Completion {idx} ---\")\n",
    "        print(output[\"generated_text\"])\n",
    "        print()\n",
    "except OSError as exc:\n",
    "    print(\"⚠️ Unable to load the text-generation pipeline:\", exc)\n",
    "    print(\"If you're working offline, download 'distilgpt2' ahead of time or point HF_HOME to a cache directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6201e",
   "metadata": {},
   "source": [
    "**Discussion prompts**\n",
    "- How does increasing the temperature change the tone of the completions?\n",
    "- What guardrails might you need before deploying an open-ended generator to students?\n",
    "- Which settings produce outputs closest to the voice you want?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201512e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Abstractive summarization\n",
    "Encoder-decoder transformer models excel at condensing long passages. Here, you will summarize an education article using [`sshleifer/distilbart-cnn-12-6`](https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
    "\n",
    "### Exercise\n",
    "1. Paste a paragraph (3-5 sentences) from an educational news article or research abstract.\n",
    "2. Run the summarization pipeline and inspect the result.\n",
    "3. Tweak `max_length` and `min_length` to balance brevity and completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab7c7eb",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "try:\n",
    "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "    # TODO: replace this article text with one you want to summarize.\n",
    "    article = \"\"\"Artificial intelligence tools are becoming more common in classrooms, helping teachers personalize assignments and\n",
    "provide instant feedback. Researchers caution that educators should combine AI suggestions with their professional judgment to ensure equitable learning experiences.\"\"\"\n",
    "\n",
    "    summary = summarizer(\n",
    "        article,\n",
    "        max_length=80,\n",
    "        min_length=30,\n",
    "        do_sample=False,\n",
    "    )[0][\"summary_text\"]\n",
    "    print(summary)\n",
    "except OSError as exc:\n",
    "    print(\"⚠️ Unable to load the summarization pipeline:\", exc)\n",
    "    print(\"Download 'sshleifer/distilbart-cnn-12-6' in advance or provide a local cache path when working offline.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3147c2d2",
   "metadata": {},
   "source": [
    "**Discussion prompts**\n",
    "- Does the model capture the main claim accurately?\n",
    "- What important nuance (if any) was lost in the summarization?\n",
    "- How might students misuse a summarization tool, and how could you guide them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92cd4e6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Zero-shot topic tagging\n",
    "Zero-shot classification lets you assign custom labels to a piece of text without additional fine-tuning. We will use [`facebook/bart-large-mnli`](https://huggingface.co/facebook/bart-large-mnli), which scores how well each label matches the text.\n",
    "\n",
    "### Exercise\n",
    "1. Create a list of 3-5 candidate labels relevant to your subject area (e.g., *assessment*, *student wellbeing*, *policy*).\n",
    "2. Supply a short passage (2-4 sentences) describing an educational scenario.\n",
    "3. Interpret the confidence scores returned by the model and discuss how you might use them to triage responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa6faed",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "try:\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "    # TODO: customize the text and labels for your own scenario.\n",
    "    scenario = \"\"\"The district is piloting a blended learning program where students rotate between online practice and small-group tutoring. Administrators are tracking engagement data to see which approach improves math outcomes.\"\"\"\n",
    "    candidate_labels = [\"personalization\", \"assessment\", \"teacher training\"]\n",
    "\n",
    "    result = classifier(scenario, candidate_labels=candidate_labels, multi_label=True)\n",
    "    for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
    "        print(f\"{label:>20}: {score:.2f}\")\n",
    "except OSError as exc:\n",
    "    print(\"⚠️ Unable to load the zero-shot classifier:\", exc)\n",
    "    print(\"Download 'facebook/bart-large-mnli' ahead of time or configure a local Hugging Face cache for offline work.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0198b6b",
   "metadata": {},
   "source": [
    "**Discussion prompts**\n",
    "- Which labels does the model consider most relevant, and why?\n",
    "- How sensitive are the results to the wording of your labels?\n",
    "- What ethical considerations arise when automatically tagging student submissions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21046e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Extension ideas\n",
    "Looking for additional challenges? Try one of the following mini-projects:\n",
    "\n",
    "- **Benchmark two models** on the same task (e.g., compare `distilgpt2` vs. `tiiuae/falcon-7b-instruct` using the [text-generation-inference](https://huggingface.co/docs/text-generation-inference/index) API).\n",
    "- **Collect student reflections** by prompting them to critique the generated outputs and propose rubric criteria.\n",
    "- **Build a simple user interface** with [Gradio](https://gradio.app/) so students can test prompts live.\n",
    "\n",
    "Feel free to adapt, expand, or remix these exercises for your classroom!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
