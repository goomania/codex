{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afcfa257",
   "metadata": {},
   "source": [
    "\n",
    "# Module B: Algorithmic Bias and Fair Machine Learning — Demo Notebook\n",
    "\n",
    "This notebook provides runnable examples for the key concepts covered in Module B. Each section pairs conceptual explanations with executable code snippets that you can use for live demonstrations or hands-on activities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e230c73",
   "metadata": {},
   "source": [
    "\n",
    "## 8.0 Module Overview\n",
    "\n",
    "We will explore:\n",
    "\n",
    "- How bias can emerge in machine learning pipelines.\n",
    "- Legal and ethical framing around discrimination.\n",
    "- Quantitative fairness metrics used to audit models.\n",
    "- Practical strategies to mitigate unfair outcomes.\n",
    "\n",
    "To keep the focus on ideas rather than data wrangling, we will work with a synthetic dataset whose structure mimics a typical tabular prediction task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e615d9f8",
   "metadata": {},
   "source": [
    "\n",
    "## 8.1 Module Introduction — Setup\n",
    "\n",
    "The first code cell imports the Python libraries we will use throughout the module and prints their versions. This is a simple smoke test to confirm that the environment (including Google Colab) has the required packages installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9aac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Library versions:\")\n",
    "print({\n",
    "    \"numpy\": np.__version__,\n",
    "    \"pandas\": pd.__version__,\n",
    "    \"scikit-learn\": __import__('sklearn').__version__,\n",
    "    \"seaborn\": sns.__version__\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ada28e9",
   "metadata": {},
   "source": [
    "\n",
    "## 8.2 Legal Definitions of Discrimination — Building a Scenario\n",
    "\n",
    "Discrimination law typically compares outcomes across protected groups. To simulate this setting, we create a dataset with an explicit sensitive attribute. The goal is to classify whether an applicant receives a positive decision (`approved = 1`) based on several features.\n",
    "\n",
    "The sensitive attribute `group` splits applicants into Group A and Group B. In this simulation, Group A enjoys a slightly higher base rate of positive outcomes, mirroring historical advantages that can propagate into automated systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8d40e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a synthetic binary classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=6,\n",
    "    n_informative=4,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=2,\n",
    "    weights=[0.55, 0.45],\n",
    "    class_sep=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['approved'] = y\n",
    "\n",
    "# Construct a sensitive attribute correlated with the target\n",
    "score = X[:, 0] + 0.5 * X[:, 1]\n",
    "threshold = np.percentile(score, 50)\n",
    "df['group'] = np.where(score >= threshold, 'Group A', 'Group B')\n",
    "\n",
    "# Introduce a mild historical bias: applicants from Group A get a small boost\n",
    "bias_mask = (df['group'] == 'Group A') & (np.random.rand(len(df)) < 0.1)\n",
    "df.loc[bias_mask, 'approved'] = 1\n",
    "\n",
    "print(df.head())\n",
    "print()\n",
    "print('Group distribution:')\n",
    "print(df['group'].value_counts(normalize=True))\n",
    "print()\n",
    "print('Approval rate by group:')\n",
    "print(df.groupby('group')['approved'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc306c55",
   "metadata": {},
   "source": [
    "\n",
    "### Visualizing Group Differences\n",
    "\n",
    "Plots help us communicate disparities to stakeholders. The following cell compares the approval rate of each group and shows a feature distribution to highlight how data characteristics can vary with group membership.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c377ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sns.barplot(\n",
    "    data=df,\n",
    "    x='group',\n",
    "    y='approved',\n",
    "    estimator=np.mean,\n",
    "    ax=axes[0],\n",
    "    palette='viridis'\n",
    ")\n",
    "axes[0].set_title('Approval Rate by Group')\n",
    "axes[0].set_ylabel('Approval Probability')\n",
    "\n",
    "sns.kdeplot(\n",
    "    data=df,\n",
    "    x='feature_0',\n",
    "    hue='group',\n",
    "    common_norm=False,\n",
    "    fill=True,\n",
    "    alpha=0.4,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('Feature 0 Distribution by Group')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8d2aa",
   "metadata": {},
   "source": [
    "\n",
    "## 8.3 Algorithmic Bias — Motivating Examples\n",
    "\n",
    "With the dataset in place, we train a logistic regression model. Logistic regression is easy to interpret and common in high-stakes domains like credit scoring and admissions.\n",
    "\n",
    "We build a pipeline that standardizes features (important for logistic regression) and fits the model. The evaluation includes accuracy metrics as well as a confusion matrix to illustrate how errors may differ across groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = feature_names\n",
    "target = 'approved'\n",
    "sensitive = 'group'\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    df[features],\n",
    "    df[target],\n",
    "    df[sensitive],\n",
    "    test_size=0.3,\n",
    "    stratify=df[[target, sensitive]],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=1000, solver='lbfgs'))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print()\n",
    "print('Classification report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix (All Applicants)')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164bc510",
   "metadata": {},
   "source": [
    "\n",
    "## 8.5 Fairness Metrics — Measuring Bias\n",
    "\n",
    "We examine several common fairness metrics:\n",
    "\n",
    "- **Statistical Parity Difference (SPD):** Difference in positive prediction rates between groups. Values near zero indicate parity.\n",
    "- **Disparate Impact (DI):** Ratio of positive prediction rates. The \"80% rule\" flags ratios below 0.8.\n",
    "- **Equal Opportunity Difference (EOD):** Difference in true positive rates (sensitivity) between groups.\n",
    "\n",
    "The helper functions below compute these metrics from predictions and ground-truth labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy.typing as npt\n",
    "\n",
    "def group_positive_rate(y_pred: npt.NDArray[np.int_], group: pd.Series, label: str) -> float:\n",
    "    mask = group == label\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return y_pred[mask].mean()\n",
    "\n",
    "\n",
    "def true_positive_rate(y_true: npt.NDArray[np.int_], y_pred: npt.NDArray[np.int_], group: pd.Series, label: str) -> float:\n",
    "    mask = group == label\n",
    "    positives = (y_true[mask] == 1)\n",
    "    if positives.sum() == 0:\n",
    "        return np.nan\n",
    "    return (y_pred[mask][positives] == 1).mean()\n",
    "\n",
    "\n",
    "def statistical_parity_difference(y_pred: npt.NDArray[np.int_], group: pd.Series, reference: str, protected: str) -> float:\n",
    "    return group_positive_rate(y_pred, group, protected) - group_positive_rate(y_pred, group, reference)\n",
    "\n",
    "\n",
    "def disparate_impact_ratio(y_pred: npt.NDArray[np.int_], group: pd.Series, reference: str, protected: str) -> float:\n",
    "    ref_rate = group_positive_rate(y_pred, group, reference)\n",
    "    prot_rate = group_positive_rate(y_pred, group, protected)\n",
    "    return prot_rate / ref_rate if ref_rate else np.nan\n",
    "\n",
    "\n",
    "def equal_opportunity_difference(y_true: npt.NDArray[np.int_], y_pred: npt.NDArray[np.int_], group: pd.Series, reference: str, protected: str) -> float:\n",
    "    return true_positive_rate(y_true, y_pred, group, protected) - true_positive_rate(y_true, y_pred, group, reference)\n",
    "\n",
    "reference_group = 'Group A'\n",
    "protected_group = 'Group B'\n",
    "\n",
    "y_pred_binary = y_pred.astype(int)\n",
    "\n",
    "target_array = y_test.values.astype(int)\n",
    "\n",
    "spd = statistical_parity_difference(y_pred_binary, group_test, reference_group, protected_group)\n",
    "di = disparate_impact_ratio(y_pred_binary, group_test, reference_group, protected_group)\n",
    "eod = equal_opportunity_difference(target_array, y_pred_binary, group_test, reference_group, protected_group)\n",
    "\n",
    "print(f\"Statistical Parity Difference (Group B - Group A): {spd:.3f}\")\n",
    "print(f\"Disparate Impact Ratio (Group B / Group A): {di:.3f}\")\n",
    "print(f\"Equal Opportunity Difference (Group B - Group A): {eod:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad39992",
   "metadata": {},
   "source": [
    "\n",
    "### Disaggregated Confusion Matrices\n",
    "\n",
    "Disaggregating errors by group makes disparities tangible. The cell below prints group-specific confusion matrices to highlight where the model struggles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f913a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for label in sorted(group_test.unique()):\n",
    "    mask = group_test == label\n",
    "    cm_group = confusion_matrix(y_test[mask], y_pred[mask])\n",
    "    print()\n",
    "    print(f'Confusion matrix for {label}:')\n",
    "    display(pd.DataFrame(\n",
    "        cm_group,\n",
    "        index=pd.Index(['Actual 0', 'Actual 1']),\n",
    "        columns=pd.Index(['Predicted 0', 'Predicted 1'])\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0104399",
   "metadata": {},
   "source": [
    "\n",
    "## 8.7 Mitigating Bias — Simple Reweighting Strategy\n",
    "\n",
    "One mitigation tactic is **reweighting**, where we increase the influence of underrepresented or disadvantaged examples during training. Here we compute inverse-probability weights by group and target label, then fit a new model using those weights. The goal is to equalize the effective sample contribution from each subgroup.\n",
    "\n",
    "> **Teaching tip:** Reweighting is a transparent intervention that maps closely to policy levers such as affirmative action or targeted outreach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b226fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = X_train.copy()\n",
    "train_df[target] = y_train\n",
    "train_df[sensitive] = group_train\n",
    "\n",
    "# Compute inverse probability weights for each (group, label) combination\n",
    "counts = train_df.groupby([sensitive, target]).size().rename('count').reset_index()\n",
    "counts['weight'] = counts['count'].sum() / (len(counts) * counts['count'])\n",
    "\n",
    "train_df = train_df.merge(counts[[sensitive, target, 'weight']], on=[sensitive, target], how='left')\n",
    "\n",
    "sample_weights = train_df['weight'].values\n",
    "\n",
    "mitigated_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(max_iter=1000, solver='lbfgs'))\n",
    "])\n",
    "\n",
    "mitigated_pipeline.fit(X_train, y_train, clf__sample_weight=sample_weights)\n",
    "mitigated_pred = mitigated_pipeline.predict(X_test)\n",
    "\n",
    "print(f\"Mitigated accuracy: {accuracy_score(y_test, mitigated_pred):.3f}\")\n",
    "print()\n",
    "print('Classification report (mitigated model):')\n",
    "print(classification_report(y_test, mitigated_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e782f7b",
   "metadata": {},
   "source": [
    "\n",
    "### Comparing Fairness Metrics Before and After Mitigation\n",
    "\n",
    "To assess whether the intervention helped, we recompute the fairness metrics for the mitigated model and compare them side-by-side with the baseline results. Small improvements illustrate the trade-offs and iterative nature of fairness work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc7630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mitigated_pred_binary = mitigated_pred.astype(int)\n",
    "\n",
    "mitigated_spd = statistical_parity_difference(mitigated_pred_binary, group_test, reference_group, protected_group)\n",
    "mitigated_di = disparate_impact_ratio(mitigated_pred_binary, group_test, reference_group, protected_group)\n",
    "mitigated_eod = equal_opportunity_difference(target_array, mitigated_pred_binary, group_test, reference_group, protected_group)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['Statistical Parity Difference', 'Disparate Impact Ratio', 'Equal Opportunity Difference'],\n",
    "    'Baseline': [spd, di, eod],\n",
    "    'Mitigated': [mitigated_spd, mitigated_di, mitigated_eod]\n",
    "})\n",
    "comparison['Change (Mitigated - Baseline)'] = comparison['Mitigated'] - comparison['Baseline']\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c4e20",
   "metadata": {},
   "source": [
    "\n",
    "## 8.8 Closing Discussion\n",
    "\n",
    "Fairness assessments are inherently contextual. This notebook showed how to:\n",
    "\n",
    "1. Surface disparities in model outcomes.\n",
    "2. Quantify those disparities with multiple metrics.\n",
    "3. Apply a lightweight mitigation strategy and evaluate its effect.\n",
    "\n",
    "Encourage students to experiment with alternative mitigation ideas—such as different weighting schemes, threshold adjustments, or feature auditing—to see how each approach shifts the fairness-accuracy balance.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
