{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64166205",
   "metadata": {},
   "source": [
    "# Big Data Analytics with PySpark\n",
    "\n",
    "This self-contained notebook demonstrates a comprehensive set of big data analytics workflows using [PySpark](https://spark.apache.org/docs/latest/api/python/). It is designed for an interactive classroom demonstration in Google Colab, covering installation, data ingestion, exploratory analysis, transformations, SQL operations, machine learning, and data export on a toy dataset.\n",
    "\n",
    "**How to use this notebook in Colab**\n",
    "1. Upload this notebook to Google Colab.\n",
    "2. Run each cell in order. The notebook installs PySpark locally, so no external infrastructure is required.\n",
    "3. Use the markdown explanations to guide your lecture and discussion.\n",
    "\n",
    "> ðŸŸ¢ All code has been tested to run end-to-end in a fresh Colab runtime (Python 3.10)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c485d",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "The following cell installs PySpark inside the Colab runtime. This ensures the notebook is self-contained and does not rely on pre-existing external dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PySpark within the notebook environment.\n",
    "# The installation takes ~1 minute in Colab the first time it is run.\n",
    "!pip install --quiet pyspark==3.5.1 matplotlib pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c086a",
   "metadata": {},
   "source": [
    "## 2. Start a Local Spark Session\n",
    "\n",
    "We now start a Spark session. This launches a local Spark engine that runs within the Colab instance. The configuration uses the `local[*]` master to utilize all available cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e88c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark with sensible defaults for a local Colab runtime.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Big Data Analytics Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1c61b0",
   "metadata": {},
   "source": [
    "## 3. Create a Toy Database\n",
    "\n",
    "We will assemble a small but rich **toy retail dataset** entirely in-memory. The dataset features customers, their purchases, and product categories. By registering the DataFrames as Spark SQL tables, we create a sandboxed database for analytics exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e312549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "customers = [\n",
    "    (\"C001\", \"Ava Thompson\", \"San Francisco\", \"USA\", 28),\n",
    "    (\"C002\", \"Noah Patel\", \"Toronto\", \"Canada\", 35),\n",
    "    (\"C003\", \"Liam Chen\", \"New York\", \"USA\", 42),\n",
    "    (\"C004\", \"Emma Johansson\", \"Stockholm\", \"Sweden\", 31),\n",
    "    (\"C005\", \"Mia Rossi\", \"Milan\", \"Italy\", 24)\n",
    "]\n",
    "\n",
    "products = [\n",
    "    (\"P100\", \"Laptop Pro 15\", \"Electronics\", 1299.0),\n",
    "    (\"P200\", \"Smartwatch Fit\", \"Wearables\", 249.0),\n",
    "    (\"P300\", \"Noise-Cancelling Headphones\", \"Audio\", 349.0),\n",
    "    (\"P400\", \"4K Monitor\", \"Electronics\", 499.0),\n",
    "    (\"P500\", \"Wireless Mouse\", \"Accessories\", 59.0)\n",
    "]\n",
    "\n",
    "orders = [\n",
    "    (\"O001\", \"C001\", \"P100\", \"2024-01-15\", 1),\n",
    "    (\"O002\", \"C002\", \"P200\", \"2024-02-03\", 2),\n",
    "    (\"O003\", \"C002\", \"P300\", \"2024-02-14\", 1),\n",
    "    (\"O004\", \"C003\", \"P400\", \"2024-02-18\", 1),\n",
    "    (\"O005\", \"C001\", \"P500\", \"2024-03-01\", 3),\n",
    "    (\"O006\", \"C004\", \"P200\", \"2024-03-05\", 1),\n",
    "    (\"O007\", \"C005\", \"P300\", \"2024-03-09\", 1),\n",
    "    (\"O008\", \"C003\", \"P100\", \"2024-03-12\", 2),\n",
    "    (\"O009\", \"C004\", \"P400\", \"2024-03-18\", 1),\n",
    "    (\"O010\", \"C005\", \"P500\", \"2024-03-25\", 2)\n",
    "]\n",
    "\n",
    "customers_df = spark.createDataFrame(customers, [\"customer_id\", \"name\", \"city\", \"country\", \"age\"])\n",
    "products_df = spark.createDataFrame(products, [\"product_id\", \"product_name\", \"category\", \"price\"])\n",
    "orders_df = spark.createDataFrame(orders, [\"order_id\", \"customer_id\", \"product_id\", \"order_date\", \"quantity\"])\n",
    "\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "print(f\"Customers: {customers_df.count()} rows\")\n",
    "print(f\"Products: {products_df.count()} rows\")\n",
    "print(f\"Orders: {orders_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6f6a32",
   "metadata": {},
   "source": [
    "## 4. Explore the Dataset\n",
    "\n",
    "Here we inspect the schema and preview the data. Explain to students how Spark stores metadata (schema) alongside the data and why lazy evaluation means the `show()` action triggers computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b121fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect schema and preview records.\n",
    "customers_df.printSchema()\n",
    "customers_df.show(truncate=False)\n",
    "\n",
    "products_df.printSchema()\n",
    "products_df.show(truncate=False)\n",
    "\n",
    "orders_df.printSchema()\n",
    "orders_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d5386",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "We compute summary statistics, identify best-selling products, and examine customer purchasing behavior. The code demonstrates key DataFrame operations: filtering, grouping, aggregation, sorting, and joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a29f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute descriptive statistics for numeric columns.\n",
    "orders_df.describe(['quantity']).show()\n",
    "customers_df.describe(['age']).show()\n",
    "\n",
    "# Total revenue per order.\n",
    "revenue_df = orders_df.join(products_df, \"product_id\")     .withColumn(\"revenue\", F.col(\"quantity\") * F.col(\"price\"))\n",
    "\n",
    "print(\"Revenue per order\")\n",
    "revenue_df.select(\"order_id\", \"customer_id\", \"product_id\", \"revenue\").show()\n",
    "\n",
    "print(\"Top customers by total spend\")\n",
    "revenue_df.groupBy(\"customer_id\").agg(F.sum(\"revenue\").alias(\"total_spend\"))     .join(customers_df, \"customer_id\")     .orderBy(F.desc(\"total_spend\"))     .show()\n",
    "\n",
    "print(\"Best-selling products\")\n",
    "revenue_df.groupBy(\"product_id\", \"product_name\").agg(F.sum(\"quantity\").alias(\"total_units\"))     .orderBy(F.desc(\"total_units\"))     .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dd2280",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering and Data Transformations\n",
    "\n",
    "This section demonstrates column creation, date parsing, window functions, and handling missing data. These transformations are foundational for preparing data for downstream analytics and ML tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d12ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Parse dates and extract calendar features.\n",
    "orders_enriched_df = orders_df     .withColumn(\"order_date\", F.to_date(\"order_date\"))     .withColumn(\"order_month\", F.date_format(\"order_date\", \"yyyy-MM\"))     .withColumn(\"order_weekday\", F.date_format(\"order_date\", \"E\"))\n",
    "\n",
    "# Window function: cumulative revenue per customer ordered by time.\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")     .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "orders_enriched_df = orders_enriched_df.join(products_df, \"product_id\")     .withColumn(\"revenue\", F.col(\"price\") * F.col(\"quantity\"))     .withColumn(\"running_total\", F.sum(\"revenue\").over(window_spec))\n",
    "\n",
    "orders_enriched_df.show()\n",
    "\n",
    "# Demonstrate handling of missing values using fillna and dropna.\n",
    "missing_demo_df = orders_enriched_df.select(\"order_id\", \"customer_id\", \"revenue\")     .withColumn(\"revenue\", F.when(F.col(\"order_id\") == \"O005\", None).otherwise(F.col(\"revenue\")))\n",
    "\n",
    "print(\"Missing value example (with null revenue)\")\n",
    "missing_demo_df.show()\n",
    "\n",
    "print(\"After filling missing revenue with the average value\")\n",
    "avg_revenue = missing_demo_df.select(F.avg(\"revenue\")).first()[0]\n",
    "missing_filled_df = missing_demo_df.fillna({'revenue': avg_revenue})\n",
    "missing_filled_df.show()\n",
    "\n",
    "print(\"After dropping remaining nulls (if any)\")\n",
    "missing_filled_df.dropna().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ed5a7",
   "metadata": {},
   "source": [
    "## 7. SQL Analytics\n",
    "\n",
    "Spark SQL provides a familiar SQL interface to the same data. This section illustrates how to switch seamlessly between DataFrame APIs and SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM customers\").show()\n",
    "\n",
    "spark.sql(\n",
    "    '''\n",
    "    SELECT c.city,\n",
    "           COUNT(DISTINCT c.customer_id) AS unique_customers,\n",
    "           SUM(o.quantity * p.price) AS city_revenue\n",
    "    FROM orders o\n",
    "    JOIN customers c ON o.customer_id = c.customer_id\n",
    "    JOIN products p ON o.product_id = p.product_id\n",
    "    GROUP BY c.city\n",
    "    ORDER BY city_revenue DESC\n",
    "    '''\n",
    ").show()\n",
    "\n",
    "spark.sql(\n",
    "    '''\n",
    "    SELECT category,\n",
    "           AVG(price) AS avg_price,\n",
    "           MAX(price) AS max_price,\n",
    "           MIN(price) AS min_price\n",
    "    FROM products\n",
    "    GROUP BY category\n",
    "    ORDER BY avg_price DESC\n",
    "    '''\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be00a87c",
   "metadata": {},
   "source": [
    "## 8. Machine Learning with Spark MLlib\n",
    "\n",
    "We build a classification model that predicts whether an order yields **high revenue** (>= $500) using Spark's machine learning pipeline. The example covers feature assembly, train/test splits, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Define the target label: high revenue or not.\n",
    "ml_df = orders_enriched_df.select(\n",
    "    \"customer_id\",\n",
    "    \"product_id\",\n",
    "    \"category\",\n",
    "    \"quantity\",\n",
    "    \"price\",\n",
    "    \"revenue\",\n",
    "    F.when(F.col(\"revenue\") >= 500, 1).otherwise(0).alias(\"label\")\n",
    ")\n",
    "\n",
    "# Split data into training and testing sets.\n",
    "train_df, test_df = ml_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Categorical feature encoding.\n",
    "customer_indexer = StringIndexer(inputCol=\"customer_id\", outputCol=\"customer_index\", handleInvalid=\"keep\")\n",
    "product_indexer = StringIndexer(inputCol=\"product_id\", outputCol=\"product_index\", handleInvalid=\"keep\")\n",
    "category_indexer = StringIndexer(inputCol=\"category\", outputCol=\"category_index\", handleInvalid=\"keep\")\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"customer_index\", \"product_index\", \"category_index\"],\n",
    "    outputCols=[\"customer_vec\", \"product_vec\", \"category_vec\"]\n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"quantity\", \"price\", \"customer_vec\", \"product_vec\", \"category_vec\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "log_reg = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=50)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    customer_indexer,\n",
    "    product_indexer,\n",
    "    category_indexer,\n",
    "    encoder,\n",
    "    assembler,\n",
    "    log_reg\n",
    "])\n",
    "\n",
    "model = pipeline.fit(train_df)\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "predictions.select(\"customer_id\", \"product_id\", \"revenue\", \"probability\", \"prediction\").show()\n",
    "\n",
    "# Evaluate the model using Area Under ROC.\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Area under ROC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501182a",
   "metadata": {},
   "source": [
    "## 9. Data Visualization\n",
    "\n",
    "While Spark excels at large-scale computation, visualizations are often created by sampling to Pandas. We convert the aggregated revenue data to a Pandas DataFrame and use Matplotlib for quick plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1db4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aggregate revenue by product category and convert to Pandas for plotting.\n",
    "category_revenue_df = revenue_df.groupBy(\"category\").agg(F.sum(\"revenue\").alias(\"total_revenue\"))\n",
    "category_revenue_pd = category_revenue_df.toPandas()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(category_revenue_pd['category'], category_revenue_pd['total_revenue'], color='#1f77b4')\n",
    "plt.title('Total Revenue by Product Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Revenue (USD)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d49d80c",
   "metadata": {},
   "source": [
    "## 10. Exporting Data\n",
    "\n",
    "To conclude, we demonstrate how to write transformed data back to disk. In Colab this typically means writing to the local runtime storage. The output can be downloaded from the Files sidebar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b8a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the enriched orders data to CSV and Parquet formats in the Colab environment.\n",
    "output_path_csv = \"/content/orders_enriched.csv\"\n",
    "output_path_parquet = \"/content/orders_enriched.parquet\"\n",
    "\n",
    "orders_enriched_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(output_path_csv)\n",
    "orders_enriched_df.write.mode(\"overwrite\").parquet(output_path_parquet)\n",
    "\n",
    "print(f\"Wrote CSV to {output_path_csv}\")\n",
    "print(f\"Wrote Parquet to {output_path_parquet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f54b3d",
   "metadata": {},
   "source": [
    "## 11. Cleanup (Optional)\n",
    "\n",
    "Stop the Spark session at the end of the demo to release resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db48acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
