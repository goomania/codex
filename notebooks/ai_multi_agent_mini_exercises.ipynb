{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60c266c0",
   "metadata": {},
   "source": [
    "\n",
    "# Multi-Agent Systems Mini Exercises\n",
    "\n",
    "This notebook introduces a lightweight cooperative multi-agent scenario that you can use to demonstrate core coordination patterns in an **AI multi-agent system**. The activities are designed for a live class where learners edit the provided scaffolding and immediately observe the effect of their changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c8484",
   "metadata": {},
   "source": [
    "\n",
    "## Learning objectives\n",
    "\n",
    "By the end of this mini workshop, learners will be able to:\n",
    "\n",
    "* Describe the difference between *local* observations and *shared* team knowledge in a cooperative setting.\n",
    "* Implement a simple message-passing strategy that lets agents coordinate their actions.\n",
    "* Evaluate whether a coordination strategy improves team performance compared with a naive baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3512c3",
   "metadata": {},
   "source": [
    "\n",
    "## How to use this notebook\n",
    "\n",
    "* Run the setup cell first to load the simple grid-world environment.\n",
    "* Skim the scenario description to understand what each agent can observe.\n",
    "* Work through the exercises in order. Each exercise has a code cell with clear `TODO` comments for students to edit.\n",
    "* At the end of the notebook there is an **example solution** that you can reveal after discussion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be0673",
   "metadata": {},
   "source": [
    "\n",
    "## Setup\n",
    "\n",
    "The only libraries needed are part of the Python standard library. Running the next cell defines a small cooperative search environment plus a helper simulator that we will reuse in the exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple\n",
    "import random\n",
    "\n",
    "Coordinate = Tuple[int, int]\n",
    "Action = str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    name: str\n",
    "    position: Coordinate\n",
    "    memory: Dict[str, float] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class CooperativeGridWorld:\n",
    "    \"\"\"A 2-agent cooperative search task on a small grid.\"\"\"\n",
    "\n",
    "    ACTION_VECTORS: Dict[Action, Coordinate] = {\n",
    "        \"up\": (-1, 0),\n",
    "        \"down\": (1, 0),\n",
    "        \"left\": (0, -1),\n",
    "        \"right\": (0, 1),\n",
    "        \"stay\": (0, 0),\n",
    "    }\n",
    "\n",
    "    def __init__(self, grid_size: Coordinate = (5, 5), target: Coordinate | None = None):\n",
    "        self.grid_size = grid_size\n",
    "        self.agent_order = (\"scout\", \"spotter\")\n",
    "        self.start_positions = {\n",
    "            \"scout\": (0, 0),\n",
    "            \"spotter\": (grid_size[0] - 1, grid_size[1] - 1),\n",
    "        }\n",
    "        self._target = target\n",
    "        self._rng = random.Random(7)\n",
    "        self.reset()\n",
    "\n",
    "    @property\n",
    "    def target(self) -> Coordinate:\n",
    "        return self._target\n",
    "\n",
    "    def reset(self, seed: int | None = None) -> Dict[str, Dict[str, float]]:\n",
    "        if seed is not None:\n",
    "            self._rng.seed(seed)\n",
    "        if self._target is None:\n",
    "            max_row, max_col = self.grid_size\n",
    "            choices = [\n",
    "                (r, c)\n",
    "                for r in range(max_row)\n",
    "                for c in range(max_col)\n",
    "                if (r, c) not in self.start_positions.values()\n",
    "            ]\n",
    "            self._target = self._rng.choice(choices)\n",
    "        self.turn = 0\n",
    "        self.agent_states = {\n",
    "            name: AgentState(name=name, position=self.start_positions[name])\n",
    "            for name in self.agent_order\n",
    "        }\n",
    "        return self._build_observations()\n",
    "\n",
    "    def step(self, actions: Dict[str, Action]):\n",
    "        self.turn += 1\n",
    "        for name, action in actions.items():\n",
    "            dr, dc = self.ACTION_VECTORS[action]\n",
    "            r, c = self.agent_states[name].position\n",
    "            new_position = (max(0, min(self.grid_size[0] - 1, r + dr)),\n",
    "                            max(0, min(self.grid_size[1] - 1, c + dc)))\n",
    "            self.agent_states[name].position = new_position\n",
    "        observations = self._build_observations()\n",
    "        reward = 0.0\n",
    "        done = any(state.position == self.target for state in self.agent_states.values())\n",
    "        if done:\n",
    "            reward = 1.0\n",
    "        return observations, reward, done\n",
    "\n",
    "    def _build_observations(self) -> Dict[str, Dict[str, float]]:\n",
    "        observations: Dict[str, Dict[str, float]] = {}\n",
    "        for name, state in self.agent_states.items():\n",
    "            row, col = state.position\n",
    "            target_row, target_col = self.target\n",
    "            observations[name] = {\n",
    "                \"row_delta\": target_row - row,\n",
    "                \"col_delta\": target_col - col,\n",
    "                \"turn\": float(self.turn),\n",
    "            }\n",
    "        return observations\n",
    "\n",
    "    def render(self) -> str:\n",
    "        grid = [[\".\" for _ in range(self.grid_size[1])] for _ in range(self.grid_size[0])]\n",
    "        tr, tc = self.target\n",
    "        grid[tr][tc] = \"T\"\n",
    "        for state in self.agent_states.values():\n",
    "            r, c = state.position\n",
    "            grid[r][c] = state.name[0].upper()\n",
    "        return \"\n",
    "\".join(\" \".join(row) for row in grid)\n",
    "\n",
    "\n",
    "def run_episode(env: CooperativeGridWorld,\n",
    "                message_fn,\n",
    "                policy_fn,\n",
    "                max_steps: int = 12,\n",
    "                verbose: bool = False):\n",
    "    \"\"\"Run a single episode and return (success, transcript).\"\"\"\n",
    "    transcript: List[str] = []\n",
    "    observations = env.reset()\n",
    "    shared_board: Dict[str, Dict[str, float]] = {name: {} for name in env.agent_order}\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        messages = {\n",
    "            name: message_fn(name, observations[name])\n",
    "            for name in env.agent_order\n",
    "        }\n",
    "        for name, payload in messages.items():\n",
    "            shared_board[name] = payload\n",
    "        actions = {\n",
    "            name: policy_fn(name, observations[name], shared_board)\n",
    "            for name in env.agent_order\n",
    "        }\n",
    "        observations, reward, done = env.step(actions)\n",
    "        if verbose:\n",
    "            transcript.append(f\"Step {step + 1}: actions={actions} reward={reward:.1f}\n",
    "{env.render()}\")\n",
    "        if done:\n",
    "            transcript.append(f\"Target found in {step + 1} steps! Reward={reward:.1f}\")\n",
    "            return True, transcript\n",
    "    transcript.append(\"Episode timed out without finding the target.\")\n",
    "    return False, transcript\n",
    "\n",
    "\n",
    "env = CooperativeGridWorld()\n",
    "print(env.render())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66d16b",
   "metadata": {},
   "source": [
    "\n",
    "The environment contains two cooperative agents:\n",
    "\n",
    "* **Scout** starts in the top-left corner. It observes the *row* distance to the hidden target.\n",
    "* **Spotter** starts in the bottom-right corner. It observes the *column* distance.\n",
    "\n",
    "On each turn both agents can move up, down, left, right, or stay put. They win as soon as either agent reaches the target location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ee4c0",
   "metadata": {},
   "source": [
    "\n",
    "### Warm-up: inspect the environment state\n",
    "\n",
    "Run the following cell to reset the environment, view the grid, and sample a single random episode. This provides a baseline before building coordinated behaviour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e07b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "success, transcript = run_episode(\n",
    "    env,\n",
    "    message_fn=lambda name, obs: {},\n",
    "    policy_fn=lambda name, obs, board: random.choice(list(CooperativeGridWorld.ACTION_VECTORS)),\n",
    "    verbose=True,\n",
    ")\n",
    "print(\"Success?\", success)\n",
    "print(\"--- Transcript ---\")\n",
    "for line in transcript:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536543d7",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 1 – Designing the message protocol\n",
    "\n",
    "Each agent has only a *partial* view of the world. The scout knows the vertical distance to the target, while the spotter knows the horizontal distance. Implement `compose_message` so that an agent shares the most useful part of its observation with the team.\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "* Return a dictionary containing numeric hints rather than a long natural-language description.\n",
    "* Include both the sign (direction) and magnitude of the helpful delta.\n",
    "* Feel free to clip or normalise values if you think it helps stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compose_message(agent_name: str, observation: Dict[str, float]) -> Dict[str, float]:\n",
    "    \"\"\"TODO: summarise what this agent knows about the target location.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    agent_name:\n",
    "        The identifier of the agent sending the message (\"scout\" or \"spotter\").\n",
    "    observation:\n",
    "        A dictionary with keys `row_delta`, `col_delta`, and `turn`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        Information that teammates can use. Keep it numeric so that policies\n",
    "        can reason over it easily.\n",
    "    \"\"\"\n",
    "    # TODO: update this placeholder implementation.\n",
    "    return {\"hint\": 0.0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c5d70d",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 2 – Turning shared knowledge into actions\n",
    "\n",
    "Create a policy that chooses an action based on both the *local* observation and the team board filled with messages from all agents. The policy should move the agent closer to the target every turn when it receives useful information.\n",
    "\n",
    "Design tips:\n",
    "\n",
    "* Use the team board to access teammate messages. You can assume the board has an entry per agent (e.g. `board[\"scout\"]`).\n",
    "* Combine local deltas with shared hints to decide which axis to move along.\n",
    "* Always return one of the valid actions listed in `CooperativeGridWorld.ACTION_VECTORS`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31498230",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def coordinated_policy(agent_name: str,\n",
    "                       observation: Dict[str, float],\n",
    "                       board: Dict[str, Dict[str, float]]) -> Action:\n",
    "    \"\"\"TODO: choose an action that uses both local observations and messages.\"\"\"\n",
    "    # TODO: replace this naive policy.\n",
    "    return \"stay\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0612b9a0",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 3 – Evaluate your strategy\n",
    "\n",
    "Once you have implemented both `compose_message` and `coordinated_policy`, run the following cell to simulate multiple episodes. Try to achieve a success rate above 80% within 12 steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(strategy_runs: int = 10) -> float:\n",
    "    wins = 0\n",
    "    for seed in range(strategy_runs):\n",
    "        env.reset(seed=seed)\n",
    "        success, _ = run_episode(env, compose_message, coordinated_policy, verbose=False)\n",
    "        wins += int(success)\n",
    "    return wins / strategy_runs\n",
    "\n",
    "success_rate = evaluate(strategy_runs=20)\n",
    "print(f\"Success rate over 20 runs: {success_rate:.0%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1798db9",
   "metadata": {},
   "source": [
    "\n",
    "### Reflect and iterate\n",
    "\n",
    "* How does the success rate change if agents only share the sign of their deltas?\n",
    "* What happens if you limit the number of turns (`max_steps`) in `run_episode`?\n",
    "* Can you design a protocol where the scout leads the exploration while the spotter only adjusts when close to the target?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af50d94",
   "metadata": {},
   "source": [
    "\n",
    "## Example solution (for instructors)\n",
    "\n",
    "The following cell contains one possible solution that reaches 100% success on the default settings. Keep it collapsed during class and expand it afterwards to review the key ideas with your learners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89015e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def solution_compose_message(agent_name: str, observation: Dict[str, float]) -> Dict[str, float]:\n",
    "    payload = {}\n",
    "    if agent_name == \"scout\":\n",
    "        payload[\"row_direction\"] = float(observation[\"row_delta\"] > 0) - float(observation[\"row_delta\"] < 0)\n",
    "        payload[\"row_distance\"] = abs(observation[\"row_delta\"])\n",
    "    elif agent_name == \"spotter\":\n",
    "        payload[\"col_direction\"] = float(observation[\"col_delta\"] > 0) - float(observation[\"col_delta\"] < 0)\n",
    "        payload[\"col_distance\"] = abs(observation[\"col_delta\"])\n",
    "    return payload\n",
    "\n",
    "\n",
    "def solution_coordinated_policy(agent_name: str,\n",
    "                                observation: Dict[str, float],\n",
    "                                board: Dict[str, Dict[str, float]]) -> Action:\n",
    "    deltas = {\n",
    "        \"row\": observation[\"row_delta\"],\n",
    "        \"col\": observation[\"col_delta\"],\n",
    "    }\n",
    "    teammate = \"spotter\" if agent_name == \"scout\" else \"scout\"\n",
    "    teammate_payload = board.get(teammate, {})\n",
    "\n",
    "    if \"row_direction\" in teammate_payload:\n",
    "        deltas[\"row\"] = teammate_payload[\"row_direction\"] * teammate_payload[\"row_distance\"]\n",
    "    if \"col_direction\" in teammate_payload:\n",
    "        deltas[\"col\"] = teammate_payload[\"col_direction\"] * teammate_payload[\"col_distance\"]\n",
    "\n",
    "    if abs(deltas[\"row\"]) >= abs(deltas[\"col\"]):\n",
    "        return \"down\" if deltas[\"row\"] > 0 else \"up\"\n",
    "    if abs(deltas[\"col\"]) > 0:\n",
    "        return \"right\" if deltas[\"col\"] > 0 else \"left\"\n",
    "    return \"stay\"\n",
    "\n",
    "\n",
    "env.reset(seed=42)\n",
    "success, transcript = run_episode(\n",
    "    env,\n",
    "    message_fn=solution_compose_message,\n",
    "    policy_fn=solution_coordinated_policy,\n",
    "    verbose=True,\n",
    ")\n",
    "print(\"Instructor solution success?\", success)\n",
    "print(\"--- Transcript ---\")\n",
    "for line in transcript:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09f4ae4",
   "metadata": {},
   "source": [
    "\n",
    "## Next steps\n",
    "\n",
    "To extend this activity you can:\n",
    "\n",
    "* Add more agents with specialised sensors to emphasise the value of scalable message protocols.\n",
    "* Introduce stochastic noise into observations so that students explore robustness.\n",
    "* Ask students to convert their deterministic policy into a learning agent using Q-learning or policy gradients.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
