{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ed4753",
   "metadata": {},
   "source": [
    "# LangChain Hugging Face Agent Mini-Project\n",
    "\n",
    "This notebook walks through building a mini-project that showcases how to combine [LangChain](https://python.langchain.com/) with a Hugging Face large language model (LLM) to create a simple AI agent. The agent can choose between multiple tools to solve tasks, which makes it an engaging classroom demonstration of *agentic* behavior.\n",
    "\n",
    "The workflow below is Colab-friendly\u2014you can open this notebook in Google Colab and run it top-to-bottom. Each step is intentionally documented so you can highlight the agent's reasoning process when presenting it live."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b333b2c",
   "metadata": {},
   "source": [
    "## 1. Install dependencies\n",
    "\n",
    "Google Colab already ships with many scientific Python packages, but we still need to install LangChain and the Hugging Face Transformers stack. The `--quiet` flag keeps the output readable during demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f731d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'langchain==0.2.7',\n",
    "    'langchain-community==0.2.7',\n",
    "    'transformers==4.39.3',\n",
    "    'accelerate==0.28.0',\n",
    "    'sentencepiece==0.1.99',\n",
    "]\n",
    "\n",
    "if os.environ.get('SKIP_COLAB_INSTALL', '').lower() in {'1', 'true', 'yes'}:\n",
    "    print('Skipping pip install because SKIP_COLAB_INSTALL is set.')\n",
    "else:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--quiet', *packages])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8458639d",
   "metadata": {},
   "source": [
    "## 2. Import libraries and configure the runtime\n",
    "\n",
    "We load the required libraries and make sure LangChain prints rich traces. The helper function below performs safe mathematical evaluations so the agent can handle numeric reasoning tasks without executing arbitrary Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9480260",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.tools import Tool\n",
    "\n",
    "try:\n",
    "    from langchain_community.llms import HuggingFacePipeline\n",
    "except ImportError:  # pragma: no cover - fallback for older LangChain releases\n",
    "    try:\n",
    "        from langchain.llms import HuggingFacePipeline  # type: ignore\n",
    "    except ImportError as exc:  # pragma: no cover\n",
    "        HuggingFacePipeline = None  # type: ignore\n",
    "        print(f\"Could not import HuggingFacePipeline: {exc}\")\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "USE_FAKE_LLM = bool(os.environ.get('USE_FAKE_LLM'))\n",
    "os.environ.setdefault('TOKENIZERS_PARALLELISM', 'false')\n",
    "\n",
    "\n",
    "def safe_eval(expression: str) -> float:\n",
    "    \"\"\"Safely evaluate basic arithmetic expressions for the calculator tool.\"\"\"\n",
    "    if not re.fullmatch(r\"[0-9+\\-*/().\\s]+\", expression):\n",
    "        raise ValueError('Expression contains unsupported characters.')\n",
    "    return eval(expression, {'__builtins__': {'abs': abs, 'round': round}}, {})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79132a6",
   "metadata": {},
   "source": [
    "## 3. Load a Hugging Face LLM\n",
    "\n",
    "We use the open-source **google/flan-t5-base** model, which runs comfortably on the free Colab T4 GPU (or even CPU). Wrapping the Transformers pipeline with LangChain's `HuggingFacePipeline` class gives us a drop-in LLM object that the agent can call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1236f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_FAKE_LLM:\n",
    "    print('Using offline demo mode \u2014 Hugging Face model will not be downloaded.')\n",
    "    llm = None\n",
    "else:\n",
    "    if HuggingFacePipeline is None:\n",
    "        raise ImportError('HuggingFacePipeline is required when USE_FAKE_LLM is not set.')\n",
    "    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "    model_name = 'google/flan-t5-base'\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    text2text_pipeline = pipeline(\n",
    "        'text2text-generation',\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "    llm = HuggingFacePipeline(pipeline=text2text_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58bb06e",
   "metadata": {},
   "source": [
    "## 4. Build domain-specific tools\n",
    "\n",
    "To demonstrate agentic behavior, we define three custom tools. Each tool solves a different class of task:\n",
    "\n",
    "1. **Calculator** \u2013 performs arithmetic with `safe_eval`.\n",
    "2. **Unit Converter** \u2013 supports a few classroom-friendly conversions.\n",
    "3. **Note Summarizer** \u2013 uses the LLM directly to condense longer text snippets.\n",
    "\n",
    "Because the tools return plain strings, they're easy to inspect while explaining how the agent reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ca5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERSION_FACTORS: Dict[str, float] = {\n",
    "    'kilometers_to_miles': 0.621371,\n",
    "    'miles_to_kilometers': 1.60934,\n",
    "    'celsius_to_fahrenheit': None,\n",
    "}\n",
    "\n",
    "\n",
    "def convert_units(query: str) -> str:\n",
    "    query = query.lower().strip()\n",
    "    match = re.search(r'-?\\d+(?:\\.\\d+)?', query)\n",
    "    if not match:\n",
    "        raise ValueError('Please include a number in your conversion request.')\n",
    "    number = float(match.group())\n",
    "    if 'kilometer' in query and 'mile' in query:\n",
    "        miles = number * CONVERSION_FACTORS['kilometers_to_miles']\n",
    "        return f\"{number} kilometers is approximately {miles:.2f} miles.\"\n",
    "    if 'mile' in query and 'kilometer' in query:\n",
    "        kilometers = number * CONVERSION_FACTORS['miles_to_kilometers']\n",
    "        return f\"{number} miles is approximately {kilometers:.2f} kilometers.\"\n",
    "    if 'celsius' in query and 'fahrenheit' in query:\n",
    "        fahrenheit = (number * 9 / 5) + 32\n",
    "        return f\"{number}\u00b0C is {fahrenheit:.1f}\u00b0F.\"\n",
    "    raise ValueError('Unsupported conversion request. Try kilometers\u2194miles or Celsius\u2194Fahrenheit.')\n",
    "\n",
    "\n",
    "def summarize_notes(text: str) -> str:\n",
    "    if USE_FAKE_LLM:\n",
    "        sentences = [sentence.strip() for sentence in text.split('.') if sentence.strip()]\n",
    "        if not sentences:\n",
    "            return 'Summary: No content provided.'\n",
    "        summary = '; '.join(sentences[:2])\n",
    "        return f'Summary: {summary}.'\n",
    "    prompt = f\"Summarize the following classroom note in 2 concise sentences:\\n\\n{text}\\n\\nSummary:\"\n",
    "    result = llm(prompt)\n",
    "    return result.strip()\n",
    "\n",
    "\n",
    "def evaluate_math(expression: str) -> str:\n",
    "    value = safe_eval(expression)\n",
    "    return f'The result is {value}.'\n",
    "\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='Calculator',\n",
    "        func=evaluate_math,\n",
    "        description=\"Useful for solving arithmetic expressions. Input should be a math expression like '23 * (4 + 2)'.\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name='Unit Converter',\n",
    "        func=convert_units,\n",
    "        description='Converts simple measurements such as kilometers to miles, miles to kilometers, or Celsius to Fahrenheit.',\n",
    "    ),\n",
    "    Tool(\n",
    "        name='Note Summarizer',\n",
    "        func=summarize_notes,\n",
    "        description='Summarizes provided lecture notes into a concise overview.',\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842250bf",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Assemble multiple agent styles\n",
    "\n",
    "LangChain ships with several opinionated agent templates. To highlight how design choices change the behavior, we'll spin up the classic **Zero-Shot ReAct** agent *and* the **Conversational ReAct** agent. The latter keeps a running chat history so the agent can refer back to previous turns\u2014perfect for classroom discussions about memory.\n",
    "\n",
    "Feel free to extend the configuration dictionary below with additional agent types if you want to experiment further in class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f792a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    agent_type: AgentType\n",
    "    description: str\n",
    "    requires_memory: bool = False\n",
    "\n",
    "\n",
    "AGENT_CHOICES = {\n",
    "    'zero_shot_react': AgentConfig(\n",
    "        agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        description='Picks tools turn-by-turn with no chat memory.',\n",
    "    ),\n",
    "    'conversational_react': AgentConfig(\n",
    "        agent_type=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "        description='Adds a conversation buffer so the agent can reference earlier turns.',\n",
    "        requires_memory=True,\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def build_agent(choice: str):\n",
    "    if USE_FAKE_LLM:\n",
    "        raise RuntimeError('Agents are disabled in offline demo mode.')\n",
    "\n",
    "    config = AGENT_CHOICES[choice]\n",
    "    agent_kwargs = {'handle_parsing_errors': True}\n",
    "\n",
    "    if config.requires_memory:\n",
    "        agent_kwargs['memory'] = ConversationBufferMemory(\n",
    "            memory_key='chat_history',\n",
    "            return_messages=True,\n",
    "        )\n",
    "\n",
    "    return initialize_agent(\n",
    "        tools,\n",
    "        llm,\n",
    "        agent=config.agent_type,\n",
    "        verbose=True,\n",
    "        **agent_kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "if USE_FAKE_LLM:\n",
    "    agents = {}\n",
    "    print('Skipping LangChain agent initialization in offline mode. Tool functions remain available for manual demos.')\n",
    "else:\n",
    "    agents = {name: build_agent(name) for name in AGENT_CHOICES}\n",
    "    for name, config in AGENT_CHOICES.items():\n",
    "        print(f\"Loaded {name}: {config.description}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd2bf2",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Demonstrate three agentic tasks (and a conversational memory bonus)\n",
    "\n",
    "Below, we run three standalone prompts through the Zero-Shot ReAct agent to showcase tool selection. Then we chat with the Conversational ReAct agent to prove it can remember earlier turns.\n",
    "\n",
    "When presenting live, pause between prompts so learners can read the reasoning traces that LangChain prints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345cc3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(agent_executor, prompts, *, title: str):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nUser: {prompt}\")\n",
    "        response = agent_executor.run(prompt)\n",
    "        print(f\"Agent: {response}\")\n",
    "\n",
    "\n",
    "zero_shot_prompts = [\n",
    "    'What is (23 * 4) + 19?',\n",
    "    'Convert 5 kilometers to miles.',\n",
    "    'Summarize this: Photosynthesis allows plants to convert light energy into chemical energy. Chlorophyll captures sunlight and drives reactions that create glucose, releasing oxygen as a byproduct.',\n",
    "]\n",
    "\n",
    "conversational_prompts = [\n",
    "    'Hi there! Please convert 3 miles to kilometers so I can note it on my class slide.',\n",
    "    'Thanks for that conversion. Can you remind me of the kilometers value you just gave me?'\n",
    "]\n",
    "\n",
    "if not agents:\n",
    "    print('Offline mode: skipping automated agent runs. You can still call the tool functions directly.')\n",
    "else:\n",
    "    run_batch(agents['zero_shot_react'], zero_shot_prompts, title='Zero-Shot ReAct tasks')\n",
    "    run_batch(agents['conversational_react'], conversational_prompts, title='Conversational ReAct dialogue')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f84df28",
   "metadata": {},
   "source": [
    "## 7. Next steps for the classroom\n",
    "\n",
    "* Ask learners to add new tools (e.g., a date planner or trivia lookup) and observe how the agent adapts.\n",
    "* Swap `google/flan-t5-base` for a more capable Hugging Face model if you have GPU resources.\n",
    "* Encourage experimentation with different agent types such as the Conversational ReAct agent.\n",
    "\n",
    "Happy teaching!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
